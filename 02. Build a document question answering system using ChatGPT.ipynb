{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/CDAC-lab/isie2023/blob/main/tutorial-notebook-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","source":["# Expose a question and answer database on top of your personal document repositories.\n","\n","This notebook is designed to demonstrate an end-to-end pipeline for processing and querying files stored in a Google Drive folder. We'll begin by accessing the folder and reading the files, which we'll then process and convert into embeddings using OpenAI. These embeddings will be stored in a ChromaDB vector database, which we'll then use to query the data using Langchain.\n","\n","## Table of Contents\n","\n","1. [Introduction and Setting Up](#section1)\n","    - Introduction to the Notebook\n","    - Installing Necessary Libraries\n","    - Importing Libraries and Dependencies\n","2. [Accessing Google Drive](#section2)\n","    - Connecting to Google Drive\n","    - Reading Files from a Google Drive Folder\n","3. [Processing and Embedding with OpenAI](#section3)\n","    - Introduction to OpenAI's API\n","    - Processing and Embedding Files\n","4. [Storing Embeddings in ChromaDB](#section4)\n","    - Introduction to ChromaDB\n","    - Storing Vector Embeddings\n","5. [Querying with Langchain](#section5)\n","    - Introduction to Langchain\n","    - Setting Up Langchain for Querying\n","    - Formulating and Executing Queries\n","6. [Conclusion and Possible Extensions](#section6)\n","    - Summary of Achievements\n","    - Potential Future Work\n","7. [References and Additional Resources](#section7)"],"metadata":{"id":"djd1PkWvR-fY"}},{"cell_type":"markdown","source":["# Introduction and Setting Up\n","\n","## Introduction to the Notebook\n","Welcome to our notebook! This project aims to process and query files stored in a Google Drive folder using OpenAI, ChromaDB, and Langchain.\n","\n","## Installing Necessary Libraries\n","In this section, we'll guide you through the installation process for all the necessary libraries that we'll use throughout this notebook. This includes libraries for interacting with Google Drive and OpenAI, and for storing and querying data with ChromaDB and Langchain.\n","\n","## Importing Libraries and Dependencies\n","Here, we'll import all the required Python libraries and dependencies. This includes standard libraries for data handling and manipulation, as well as libraries specific to our pipeline such as the API wrappers for Google Drive, OpenAI, ChromaDB, and Langchain.\n"],"metadata":{"id":"WUdq1WGeSE9G"}},{"cell_type":"markdown","source":["##Install Libraries"],"metadata":{"id":"RYBvGUEXrCRV"}},{"cell_type":"code","source":["!pip install openai\n","!pip install langchain\n","!pip install pypdf\n","!pip install tiktoken\n","!pip install chromadb\n","!pip install python-magic\n","!pip install pdf2image\n","!apt-get install poppler-utils"],"metadata":{"id":"UhCknCegQDge"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Import Libraries"],"metadata":{"id":"JEbvDmQnrNCt"}},{"cell_type":"code","source":["#libraries for google drive authentication\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.vectorstores import Chroma\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain import OpenAI, VectorDBQA\n","from langchain.chat_models import ChatOpenAI\n","from langchain.document_loaders import PyPDFLoader\n","import matplotlib.pyplot as plt\n","# import magic\n","import os\n","import nltk\n","# import pytesseract\n","import textwrap\n","from pdf2image import convert_from_path"],"metadata":{"id":"VSgvRG44RD8P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Set OpenAI Key"],"metadata":{"id":"PuP1bo73rQMO"}},{"cell_type":"code","source":["import os\n","os.environ[\"OPENAI_API_KEY\"] = \"give your key\""],"metadata":{"id":"5D5W_BeBZf5X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Accessing Google Drive\n","\n","## Connecting to Google Drive\n","In this section, we'll guide you through the process of connecting to Google Drive from this notebook. This involves authenticating with your Google account and setting up the necessary permissions.\n","\n","## Reading Files from a Google Drive Folder\n","Once we're connected to Google Drive, we'll show you how to access a specific folder and read the files within it. We'll also discuss how to handle different types of files and any potential issues that might arise."],"metadata":{"id":"Og9AAs_GSJr-"}},{"cell_type":"markdown","metadata":{"id":"agvC1uCXIxCS"},"source":["###Download documents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IFsGxuBELUR6"},"outputs":[],"source":["#authenticate with you google drive credentials\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","# This is the file ID of the data set, this will download the datafile from the shared location\n","file_id = '1F4ujHU6hzj4mIJOBmkSLE7srYbWAhRb4'\n","sample_data = drive.CreateFile({'id':file_id})\n","sample_data.GetContentFile('prompt_engineering.pdf')"]},{"cell_type":"markdown","metadata":{"id":"QdfoRTZcJk1K"},"source":["### Load documents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6TrMINtbJriy"},"outputs":[],"source":["loader = PyPDFLoader(\"prompt_engineering.pdf\")\n","pages = loader.load_and_split()\n","text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n","docs = text_splitter.split_documents(pages)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6evJrrORpO7t"},"outputs":[],"source":["def display_document(path):\n","  images = convert_from_path(path)\n","  print(\"Total number of pages\",len(images))\n","  _, axs = plt.subplots(10,4, figsize=(30, 60),squeeze=False)\n","  axs = axs.flatten()\n","  for img, ax in zip(images, axs):\n","      ax.set_xticks([])\n","      ax.set_yticks([])\n","      ax.imshow(img)\n","  # use tight_layout\n","  _.tight_layout()\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Vmi8DKwvEGLDvd4K0z-m9qtF0nISNvqn"},"executionInfo":{"elapsed":34301,"status":"ok","timestamp":1696849323472,"user":{"displayName":"Gihan Gamage","userId":"03983047858725985766"},"user_tz":-660},"id":"7-VTBXgWrCZd","outputId":"b83cc092-f563-4b44-cacd-92cb3d3f1e43"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["#display the pdf\n","display_document(\"prompt_engineering.pdf\")"]},{"cell_type":"markdown","source":["##Chunking documents"],"metadata":{"id":"CnpASa2UrbUu"}},{"cell_type":"code","source":["char_text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n","doc_texts = char_text_splitter.split_documents(docs)"],"metadata":{"id":"dGOOj7jKRxPE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Processing and Embedding with OpenAI\n","\n","## Introduction to OpenAI's API\n","OpenAI's API allows us to process our files and convert them into vector embeddings. In this section, we'll provide a brief introduction to the API and explain how we'll use it in our pipeline.\n","\n","## Processing and Embedding Files\n","Here, we'll walk you through the process of sending our files to the OpenAI API, receiving vector embeddings in return, and preparing these embeddings for storage in ChromaDB."],"metadata":{"id":"KK1bP4bRSN5R"}},{"cell_type":"markdown","source":["##Extract OpenAI embeddings to document chunks"],"metadata":{"id":"3c9OH8zXruO3"}},{"cell_type":"code","source":["openAI_embeddings = OpenAIEmbeddings(openai_api_key=os.environ['OPENAI_API_KEY'])"],"metadata":{"id":"m1EA0RpXR2Mu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Create vector store"],"metadata":{"id":"l6kwxRaqr3Xv"}},{"cell_type":"code","source":["vStore = Chroma.from_documents(doc_texts, openAI_embeddings)"],"metadata":{"id":"ser2CuUwr8MP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Initialize VectorDBQA Chain from LangChain"],"metadata":{"id":"iSJnBHQ4r6zv"}},{"cell_type":"code","source":["model = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=vStore)"],"metadata":{"id":"ATtB6jlYVF6v","outputId":"869020d0-10a5-4469-f5c1-e9ce3cadff28","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696849351208,"user_tz":-660,"elapsed":4,"user":{"displayName":"Gihan Gamage","userId":"03983047858725985766"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval_qa/base.py:251: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["# Storing Embeddings in ChromaDB\n","\n","## Introduction to ChromaDB\n","ChromaDB is a high-performance vector database that we'll use to store our embeddings. In this section, we'll explain what ChromaDB is and why it's useful in our pipeline.\n","\n","## Storing Vector Embeddings\n","Once we have our vector embeddings, it's time to store them in ChromaDB. We'll show you how to send the embeddings to ChromaDB, ensuring```markdown\n","that they're properly indexed and ready for querying.\n","\n","# Querying with Langchain\n","\n","## Introduction to Langchain\n","Langchain provides a natural language interface for querying our vector data. In this section, we'll provide an introduction to Langchain and explain how it fits into our pipeline.\n","\n","## Setting Up Langchain for Querying\n","Before we can start querying, we need to set up Langchain. This section will guide you through the process of setting up Langchain to work with our vectorized data.\n","\n","## Formulating and Executing Queries\n","With Langchain set up, we can now formulate and execute queries on our data. We'll walk you through the process of creating a query, sending it to Langchain, and interpreting the results."],"metadata":{"id":"md3yY59ESUk2"}},{"cell_type":"markdown","source":["##Question Anwering"],"metadata":{"id":"cUH6Bst1r7Y0"}},{"cell_type":"code","source":["question = \"What is prompt engineering?\"\n","response = model.run(question)\n","print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"USr69aohVPPW","outputId":"cc16299a-a785-4b0c-b7c5-812a27a83dc9","executionInfo":{"status":"ok","timestamp":1696849359722,"user_tz":-660,"elapsed":3852,"user":{"displayName":"Gihan Gamage","userId":"03983047858725985766"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Prompt engineering is a process of creating a set of prompts, or questions, that are used to guide the user toward a desired outcome. It is an effective tool for designers to create user experiences that are easy to use and intuitive. This method is often used in interactive design and software development, as it allows users to easily understand how to interact with a system or product.\n"]}]},{"cell_type":"code","source":["question = \"List 4 elements of a prompt and explain\"\n","response = model.run(question)\n","print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nP-PeC_l0Fey","outputId":"a06dda40-5f24-4be5-ac16-b9384c232ca9","executionInfo":{"status":"ok","timestamp":1696849367592,"user_tz":-660,"elapsed":2910,"user":{"displayName":"Gihan Gamage","userId":"03983047858725985766"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Elements of a prompt include instructions, context, input data, and output indicator. Instructions tell the user what they need to do, context provides information the user needs to complete the task, input data is the information the user needs to provide, and output indicator tells the user what the result of their input should be.\n"]}]},{"cell_type":"markdown","source":["# Conclusion and Possible Extensions\n","\n","## Summary of Achievements\n","In this section, we'll summarize what we've achieved in this notebook, from reading files in a Google Drive folder to querying the content using Langchain.\n","\n","## Potential Future Work\n","The pipeline we've built has many potential extensions and improvements. Here, we'll discuss some possibilities for future work, such as refining the processing and embedding process or expanding the types of queries we can handle.\n","\n","# References and Additional Resources\n","To wrap up the notebook, we'll provide a list of references and additional resources that you can use to further explore the topics covered in this notebook."],"metadata":{"id":"xo6hlXOySbcM"}}]}