{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week_05_Natural_Language_Processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8opw2UZFaQv"
      },
      "source": [
        "# Natural Language Processing with Python\n",
        "\n",
        "In this workshop we will use the [Sentiment Labelled Sentences Data Set](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)  that contains 500 positive and 500 negative sentences from imdb.com, amazon.com and yelp.com.  \n",
        "\n",
        "* First, we will explore preliminary text analytics and text pre-processing .  \n",
        "* Second, we will evaluate different feature extraction mechanisms for text.  \n",
        "* Third, we will evaluate a simple text classification for Amazon review dataset, and an advanced deep neural network for classification accuracy improvement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_z-LSYHJVxx"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "Load the reviews.csv file downloaded from LMS to the Google Colab file repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XCT8rJKJv0j"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWP9fyKaNRkN"
      },
      "source": [
        "# Dataset reveiws.csv MUST be uploaded to Google Colab before executing this line\n",
        "df = pd.read_csv('tp6_reviews.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KblPiHu6ZfT1"
      },
      "source": [
        "View a summary of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlFws42RNS3Q"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uroM_ht8N9Xt"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FlpqeMJ4dyW"
      },
      "source": [
        "Check what are the sources of data available in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqjbA0GZ4Zlc"
      },
      "source": [
        "df['source'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFPeBYVw3gDz"
      },
      "source": [
        "## Preliminary analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3xamOU23qJ0"
      },
      "source": [
        "***Pandas dataframe's df.apply() function***  \n",
        "* This allow the users to pass a function and apply it on every single value of the Pandas series, i.e., column. [API documentation.](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html)\n",
        "*  Efficient way to update values in a dataframe column\n",
        "\n",
        "In the following example, \n",
        "\n",
        "\n",
        "*   Count the number of words in each sentence\n",
        "*   Assign the word count to a new attribute  named 'word_count'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEYoC-bJ6Uj5"
      },
      "source": [
        "def word_counter(document):\n",
        "  split_word = str(document).split(\" \") # split by white space\n",
        "  word_count = len(split_word) # count the words\n",
        "  return word_count\n",
        "\n",
        "df['word_count_function'] = df['sentence'].apply(word_counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIyW2QaG6pbS"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNKXacvx60GL"
      },
      "source": [
        "Same above function can be achieved through a simple lambda function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6767ZK4ZOAhd"
      },
      "source": [
        "df['word_count'] = df['sentence'].apply(lambda x: len(str(x).split(\" \")))\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RYDf5StaS5s"
      },
      "source": [
        "Similarly, count the number of characters of each sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrT9GLxt5RgZ"
      },
      "source": [
        "df['char_count'] = df['sentence'].str.len()  # Includes the spaces\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmygJjvfabcd"
      },
      "source": [
        "Calculate the average word length for each sentence.\n",
        "\n",
        "*   First, construct a method (avg_word()) which takes a sentence, split the sentence to words, then calculate the average word length.\n",
        "*   Using pandas dataframe apply() function and avg_word() method, calculate the average word length\n",
        "*  Assign the value to new column names 'avg_word'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-FyuyvnbMS3"
      },
      "source": [
        "def avg_word(sentence):\n",
        "  words = sentence.split() # split the sentence into words\n",
        "  avg_of_words = (sum(len(word) for word in words)/len(words))\n",
        "  return avg_of_words\n",
        "\n",
        "df['avg_word'] = df['sentence'].apply(avg_word)\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7NajYVd8t-r"
      },
      "source": [
        "## Text pre-processing\n",
        "\n",
        "Pre-processing is mandatory for most text analytics tasks, as text in its raw format is unstructured and noisy. \n",
        "\n",
        "In the following snippets you will run several pre-processing steps.  \n",
        "\n",
        "**Please note that pre-processing is to be used with clear understanding of the expected outcome of text analytics, as each pre-processing step is not relevant or applicable to every NLP task.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ56xCM0dxLy"
      },
      "source": [
        "Uppercase and lowercase characters are used for clarity in human communication. However, for a machine such distinction would create unnecessary complexities. Therefore, we transform all characters to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gowXmkYk50h_"
      },
      "source": [
        "df['sentence'] = df['sentence'].str.lower()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXvngUDDelRX"
      },
      "source": [
        "Same with punctuation marks, we remove all using [regular expressions](https://www.w3schools.com/python/python_regex.asp) .  \n",
        "**Regular Expressions** - A regular expression is a special sequence of characters that helps you match or find other strings or sets of strings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHsiGbbY9Hqg"
      },
      "source": [
        "# This regular expression only keeps words and characters\n",
        "df['sentence'] = df['sentence'].str.replace('[^\\w\\s]','')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbhTrjC6lD1D"
      },
      "source": [
        "### Remove digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-PafqgZe7x7"
      },
      "source": [
        "For a sentiment analytics task, numbers or digits are not needed. Thus, we remove digits from the text dataset. \n",
        "\n",
        "However, for other tasks, numbers may be needed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKRv9btDlLNz"
      },
      "source": [
        "def remove_digits(sent):\n",
        "  return \" \".join(w for w in sent.split() if not w.isdigit())\n",
        "\n",
        "df['sentence'] = df['sentence'].apply(remove_digits)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_xppk798hrz"
      },
      "source": [
        "Demonstrate of the remove digit function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntqhwksm8T9M"
      },
      "source": [
        "sample_text = 'Covid 19 is spreading fast'\n",
        "print(remove_digits(sample_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlxTKunt8lyT"
      },
      "source": [
        "What does \"\".join() means?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2kfwcVk8oyi"
      },
      "source": [
        "word_list = [\"Covid\", \"is\", \"spreading\", \"fast\"]\n",
        "sentence = \"     \".join(word_list)\n",
        "print(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwh1imSm-BC3"
      },
      "source": [
        "### Remove Stopwords\n",
        "\n",
        "[Stopwords](https://en.wikipedia.org/wiki/Stop_words) are deemed irrelevant for NLP purposes because they occur frequently in the language. Therefore, we will omit the stopwords as a pre-processing step. For this, we will use [NLTK](https://www.nltk.org/) library here.\n",
        "\n",
        "**NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) specifically for the English language written in  Python.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1J6fC9491k7"
      },
      "source": [
        "# Load NLTK library\n",
        "import nltk\n",
        "\n",
        "# Download the stopwords to the nltk library\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the stopwords\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecLfjQgmgYpk"
      },
      "source": [
        "Have a look at the stopwords indexed in the NLTK library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkpvQNV5-N2g"
      },
      "source": [
        "stop = stopwords.words('english')\n",
        "print(stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H73Xr48-cHAS"
      },
      "source": [
        "Remove unwanted stop words from the NLTK stop word list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPVzEQ_8O0UR"
      },
      "source": [
        "stop.remove('not')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO8HTTOXPYyy"
      },
      "source": [
        "all_words_i_want = ['had', \"has\"]\n",
        "for w in all_words_i_want:\n",
        "  stop.remove(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPugCcb3gzMj"
      },
      "source": [
        "Remove stopwords from the sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_HwAnis-Ut1"
      },
      "source": [
        "df['sentence'] = df['sentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUol6pfo_MEL"
      },
      "source": [
        "### Common and rare word analysis\n",
        "\n",
        "Aside from stopwords, some words appear rarely (only once or twice) in an entire body of text. \n",
        "Based on the analytics requirement, you can decide whether to keep or remove, and at what intensity/scale to remove."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iyA7bBDg_hB"
      },
      "source": [
        "In order to do this, first we have to construct a word frequency dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHCEJC3V-wG0"
      },
      "source": [
        "word_frequency = pd.Series(' '.join(df['sentence']).split()).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tsu7TJOhGGK"
      },
      "source": [
        "List the top 10 common words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP6ids9d_ldS"
      },
      "source": [
        "# Top common words\n",
        "word_frequency[:10]  # get top 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEIsBYCGhLlY"
      },
      "source": [
        "List the top 10 rare words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvUQt5hRAM13"
      },
      "source": [
        "# least common words\n",
        "word_frequency[-10:]  # get top 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7hd1zK8AXSl"
      },
      "source": [
        "### Spelling correction\n",
        "\n",
        "To correct misspelt words, we will use [textblob library](https://textblob.readthedocs.io/en/dev/) library. Keep in mind that corrections are always bound by the dictionary that you would use, and it may not account for context (their vs there).\n",
        "\n",
        "Due to the time complexity of spell-checking an entire corpus, in this exercise, we will use spell-check for just one example. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH_MOO58iity"
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf9rFrJpARWH"
      },
      "source": [
        "# Do not run this line of code.\n",
        "# Following line of code will correct spellings of all the sentences in the dataset.\n",
        "# df['sentence'] = df['sentence'].apply(lambda x: str(TextBlob(x).correct()))   # This will take a long time. Thus, we will show an seperate example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6ERDidUivTx"
      },
      "source": [
        "Spelling correction example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2U3yRK5QqkA"
      },
      "source": [
        "def correct_word(word):\n",
        "  return str(TextBlob(word).correct())\n",
        "\n",
        "print(correct_word('bisness'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6D1dM55A_5l"
      },
      "source": [
        "incorrect_text = 'bisness anlytis is an itant skil seit for any organizaton'\n",
        "\n",
        "func = lambda x: str(TextBlob(x).correct())\n",
        "print(incorrect_text)\n",
        "print(str(TextBlob(incorrect_text).correct()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F_w641sCItw"
      },
      "source": [
        "### Stemming\n",
        "\n",
        "[Stemming](https://en.wikipedia.org/wiki/Stemming) is the removal of prefix, suffix etc, to derive the base form of a word. We will use the NLTK library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MxoI8j4Bf93"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stemming_function(sent):\n",
        "  word_list = sent.split()\n",
        "  stemmed_word_list = [stemmer.stem(word) for word in word_list]\n",
        "  stemmed_sentence = \" \".join(stemmed_word_list)\n",
        "  return stemmed_sentence\n",
        "\n",
        "df['sentence_stemmed'] = df['sentence'].apply(stemming_function)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuZ8AVKvDKQG"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "[Lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html), unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.  \n",
        "  \n",
        "We will use  Wordnet for the lemmatization. Thus, we need to download Wordnet to the nltk library.\n",
        "\n",
        "WordNet is a lexical database for the English language. It groups English words into sets of synonyms called synsets, provides short definitions and usage\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8qJOjLwDhzB"
      },
      "source": [
        "# Download wordnet\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjCuPX1zjrWD"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmtizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRdYHVNQCiB9"
      },
      "source": [
        "def lemmatize_function(sent):\n",
        "  word_list = sent.split()\n",
        "  lemma_word_list = [lemmtizer.lemmatize(word) for word in word_list]\n",
        "  lemma_sentence = \" \".join(lemma_word_list)\n",
        "  return lemma_sentence\n",
        "\n",
        "df['sentence_lemmatized'] = df['sentence'].apply(lemmatize_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKCDAK0PkCeW"
      },
      "source": [
        "Display original pre-processed sentence, stemmed sentence and lemmatized sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbfKn2OnkDgl"
      },
      "source": [
        "df[['sentence', 'sentence_stemmed', 'sentence_lemmatized']].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6RTwgzzwsJg"
      },
      "source": [
        "Stemmed algorithm seems to be working better in this case when compared to the lemmatization. It is recommended to observe the results after pre-processing tasks to understand the performance of the third party libraries we are using in pre-processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILB7exVHEY1y"
      },
      "source": [
        "## Text Feature Extraction\n",
        "\n",
        "In a numeric dataset (e.g., house price dataset, titanic survival dataset, dungaree dataset), we had numeric and categorical variables, which we transformed to numeric values for predictive analytics. Those numeric variables are called numeric features in the datasets. Similarly, for NLP, we need to derive features from text data in numerical format because machines can only understand numeric representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FlXXX-cfHZa"
      },
      "source": [
        "### N-Grams\n",
        "\n",
        "An [n-gram](https://en.wikipedia.org/wiki/N-gram) is a contiguous sequence of n items from a given sample of text or speech. They are basically a set of co-occuring words within a given window. When computing the n-grams, the shift is one-step forward (although you can move X words forward in more advanced scenarios). For example, for the sentence \"The cow jumps over the moon\". If N=2 (known as bigrams), then the ngrams would be:\n",
        "* the cow\n",
        "* cow jumps\n",
        "* jumps over\n",
        "* over the\n",
        "* the moon\n",
        " \n",
        "We will use NLTK ngrams and word_tokenizer libraries for n-gram feature extraction.\n",
        "\n",
        "Note: Need to download punkt resource for nltk for work tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inAc6aR2lwEg"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJYeiIrClzp7"
      },
      "source": [
        "First we define the value for *n*, in n-gram representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb18qzswl-Bq"
      },
      "source": [
        "n = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX6iMftbmDPO"
      },
      "source": [
        "Following n_grams() method will take a sentence and construct a list of n-grams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKPsz4T_l_ZU"
      },
      "source": [
        "def n_grams(text):\n",
        "  if len(word_tokenize(text)) < 3:\n",
        "    return []\n",
        "  n_grams = ngrams(word_tokenize(text), n)\n",
        "  return [' '.join(grams) for grams in n_grams]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zQdw-4pS8ws"
      },
      "source": [
        "txt = \"you want to exclude some stop word being getting ignored\"\n",
        "print(n_grams(txt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2EEAEptmPrn"
      },
      "source": [
        "Derive n-grams (n=3) for our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj2gxPH7DY84"
      },
      "source": [
        "df['3_grams'] = df['sentence'].apply(lambda x: n_grams(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XniThjwmV-r"
      },
      "source": [
        "Display original sentence and n-grams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQUU-YcdmNTD"
      },
      "source": [
        "df[['sentence', '3_grams']].head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RJBlyLjh0Zq"
      },
      "source": [
        "Based on above results (e.g., record 16) you can see that if there are only 2 words, the 3-grams would result no n-grams.  \n",
        "Thus, you may try to derive n-grams with *n=2*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhlvf58tqq3C"
      },
      "source": [
        "### Bag of words\n",
        "\n",
        "[Bag of words](https://machinelearningmastery.com/gentle-introduction-bag-words-model/) is a simple text feature extraction mechanism.   \n",
        "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
        "* A vocabulary of known words.  \n",
        "* A measure of the presence of known words.  \n",
        "\n",
        "We will use [CountVectorizer library](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) on sklearn for bag-of-words model creation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDsvQka_oJGI"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBazktNpoPpH"
      },
      "source": [
        "You may refer to [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) API for detailed description about the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKuTRNfYmqV0"
      },
      "source": [
        "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1), analyzer = \"word\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frDRiREJokx8"
      },
      "source": [
        "Transform lemmatized senteces into bag-of-words model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8_sv_6eeZEJ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1NwVXryrEj2"
      },
      "source": [
        "X_bow = bow.fit_transform(df['sentence_stemmed'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLyqMA4CoxhA"
      },
      "source": [
        "The X_bow would result in a term-document matrix.  \n",
        "e.g., Output format:  (sentence_id, vocabulary_dictionary_id) count\n",
        "* sentence_id - sentence id in the dataframe\n",
        "* vocabulary_dictionary_id - id of the particular word in the bag of words model dictionary\n",
        "* count - count of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTedh3MhT6MT"
      },
      "source": [
        "df['sentence_stemmed'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imw2JQaYrM73"
      },
      "source": [
        "print(X_bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Fxf1C1iPN3"
      },
      "source": [
        "### Term Frequency - Inverse Document Frequecy (TF-IDF)\n",
        "\n",
        "[Term frequency–inverse document frequency](https://www.kdnuggets.com/2018/08/wtf-tf-idf.html), is a numerical statistic that is intended to reflect how important a word is to a document in a collection. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\n",
        "\n",
        "We will use [feature extraction module](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) of the sklearn library for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFR61d5ehktG"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzNi0OmbqSs7"
      },
      "source": [
        "Construct TF-IDF using the lemmatized senteces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leDbV3eVjLUK"
      },
      "source": [
        "tf_idf = vectorizer.fit_transform(df['sentence_stemmed'])  # as the text data, we will use lemmatized sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kX79IoAqbdb"
      },
      "source": [
        "Display the list of all the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzBSDPTKktzH"
      },
      "source": [
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8oekEhSlt4r"
      },
      "source": [
        "Here you see there are quite many text that includes a number (digit).  \n",
        "In one of the pre-processing steps, we removed all the words/text that are only digits, but not combined.  \n",
        "You might want to remove these as well...  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSgn4DPGqtEz"
      },
      "source": [
        "A comparison of TF-IDF values with respect to lemmatized sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vUTSbDdk0wL"
      },
      "source": [
        "print(df['sentence_lemmatized'].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb7xxL-LlIul"
      },
      "source": [
        "print(tf_idf[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kE8A2SBmxdf"
      },
      "source": [
        "In the feature vector row (e.g., (0, 4843)), the first digit refers to the sentence row (i.e., first datarow).  \n",
        "The second digit is the index of alphebitically ordered word list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eNVD0_wGYoP"
      },
      "source": [
        "### Sentiment Analysis\n",
        "\n",
        "Sentiment analysis is basically the process of determining the attitude or the emotion of the writer, i.e., whether it is positive or negative or neutral.\n",
        "\n",
        "We will use the Textblob library. The sentiment function of textblob returns the polarity of the sentence, i.e., a float value which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7Y1RR38HFs3"
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPElhKYqU2-r"
      },
      "source": [
        "doc1 = \"wow\"\n",
        "TextBlob(doc1).sentiment.polarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTyI82q-rRZM"
      },
      "source": [
        "Derive sentiment of each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB_i-p0QG8WT"
      },
      "source": [
        "df['sentiment'] = df['sentence_lemmatized'].apply(lambda x: TextBlob(x).sentiment.polarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxndvXWrrbdK"
      },
      "source": [
        "Display original sentece with respect to its sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBZ-FeKdG8c2"
      },
      "source": [
        "print(df[['sentence_lemmatized', 'sentiment']][:40])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acrCV8JVVjdx"
      },
      "source": [
        "## Text Classification\n",
        "\n",
        "We will explore few text classification approaches to classify the review data as either positive (1) or negative (0).  \n",
        "Here we will only use the amazon reviews (1000 reviews) for the workshop. (You may use yelp and imdb review data seperately evaluate the approaches.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSsjUBYZrl-y"
      },
      "source": [
        "Previously, we conducted all the pre-processing steps to the entire 3 datasets (amazon, yelp and imdb). This for text classification we will filter only the reviews from amazon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl8comQ3V0zJ"
      },
      "source": [
        "df_amazon = df.loc[df['source'] == 'amazon']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm2-8QfKb90u"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBIhvHkmWdku"
      },
      "source": [
        "df_amazon.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0JIXz4gr4A0"
      },
      "source": [
        "Split train/validation data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hNVAhPGXmSi"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(df_amazon['sentence_lemmatized'], df_amazon['label'], test_size=0.3, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19jv2X2kZV3B"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kPqYktYsOxS"
      },
      "source": [
        "We will use the Bag of Words model as text features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOLyQxTqWfWB"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aw3BCr0sYLQ"
      },
      "source": [
        "Construct the bag of words model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgvIlfHhsYby"
      },
      "source": [
        "bow = CountVectorizer(min_df=0, lowercase=False)\n",
        "bow.fit(sentences_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mj1IC8CsgTW"
      },
      "source": [
        "Fit the train and test sentences to transform them to bag-of-word features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNZIJTzdXCbE"
      },
      "source": [
        "X_train = bow.transform(sentences_train)\n",
        "X_test  = bow.transform(sentences_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ3_5XCOspgD"
      },
      "source": [
        "Use a logistic regression model for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xqy_u4hYT2i"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvhA_f2FtDeU"
      },
      "source": [
        "Evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhqBSmAfYg6I"
      },
      "source": [
        "score_train = classifier.score(X_train, y_train)\n",
        "score_test = classifier.score(X_test, y_test)\n",
        "print('Train accuracy: {:.2f}%'.format(score_train*100))\n",
        "print('Test accuracy: {:.2f}%'.format(score_test*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geKjAtjGtJ6g"
      },
      "source": [
        "What can you say about bias and variance of this model?  \n",
        "- Low bias and high variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq_TLlEUKQeC"
      },
      "source": [
        "Try few examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riamqEZjGhrj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a77f47df-18e4-4aa1-b2ef-f5c1c27d3d81"
      },
      "source": [
        "testing = \"happy customer\"\n",
        "vector_representation = bow.transform([testing])\n",
        "prediction = classifier.predict(vector_representation)[0]\n",
        "\n",
        "if prediction == 1:\n",
        "   print(\"Positive Review\")\n",
        "else:\n",
        "   print(\"Negative Review\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive Review\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shvn4nHzhIRr"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "You may conduct similar classification exercises for yelp and imdb datasets."
      ]
    }
  ]
}