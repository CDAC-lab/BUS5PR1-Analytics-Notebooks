{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 05 Natural Language Processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8opw2UZFaQv",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Processing with Python\n",
        "\n",
        "In this workshop we will use the [Sentiment Labelled Sentences Data Set](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)  that contains 500 positive and 500 negative sentences from imdb.com, amazon.com and yelp.com.  \n",
        "\n",
        "* First, we will explore preliminary text analytics and text pre-processing .  \n",
        "* Second, we will evaluate different feature extraction mechanisms for text.  \n",
        "* Third, we will evaluate a simple text classification for Amazon review dataset, and an advanced deep neural network for classification accuracy improvement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_z-LSYHJVxx",
        "colab_type": "text"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "Load the reviews.csv file downloaded from LMS to the Google Colab file repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XCT8rJKJv0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWP9fyKaNRkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset reveiws.csv MUST be uploaded to Google Colab before executing this line\n",
        "df = pd.read_csv('tp6_reviews.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KblPiHu6ZfT1",
        "colab_type": "text"
      },
      "source": [
        "View a summary of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlFws42RNS3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "dff194fe-5e98-408d-957d-5f45403bb16d"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2748 entries, 0 to 2747\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   sentence  2748 non-null   object\n",
            " 1   label     2748 non-null   int64 \n",
            " 2   source    2748 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 64.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uroM_ht8N9Xt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "44f1d47d-5ac2-476a-c6ab-c6c2d6ec7430"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  label source\n",
              "0                           Wow... Loved this place.      1   yelp\n",
              "1                                 Crust is not good.      0   yelp\n",
              "2          Not tasty and the texture was just nasty.      0   yelp\n",
              "3  Stopped by during the late May bank holiday of...      1   yelp\n",
              "4  The selection on the menu was great and so wer...      1   yelp"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FlpqeMJ4dyW",
        "colab_type": "text"
      },
      "source": [
        "Check what are the sources of data available in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqjbA0GZ4Zlc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8083c76-e7c3-4a9d-c115-7dcf315adcbf"
      },
      "source": [
        "df['source'].unique()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['yelp', 'amazon', 'imdb'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFPeBYVw3gDz",
        "colab_type": "text"
      },
      "source": [
        "## Preliminary analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3xamOU23qJ0",
        "colab_type": "text"
      },
      "source": [
        "***Pandas dataframe's df.apply() function***  \n",
        "* This allow the users to pass a function and apply it on every single value of the Pandas series, i.e., column. [API documentation.](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html)\n",
        "*  Efficient way to update values in a dataframe column\n",
        "\n",
        "In the following example, \n",
        "\n",
        "\n",
        "*   Count the number of words in each sentence\n",
        "*   Assign the word count to a new attribute  named 'word_count'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEYoC-bJ6Uj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_counter(word):\n",
        "  split_word = str(word).split(\" \") # split by white space\n",
        "  word_count = len(split_word) # count the words\n",
        "  return word_count\n",
        "\n",
        "df['word_count_function'] = df['sentence'].apply(word_counter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIyW2QaG6pbS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b57f2e45-508b-4672-e382-51b1c071ba7f"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>source</th>\n",
              "      <th>word_count_function</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...  word_count_function\n",
              "0                           Wow... Loved this place.  ...                    4\n",
              "1                                 Crust is not good.  ...                    4\n",
              "2          Not tasty and the texture was just nasty.  ...                    8\n",
              "3  Stopped by during the late May bank holiday of...  ...                   15\n",
              "4  The selection on the menu was great and so wer...  ...                   12\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNKXacvx60GL",
        "colab_type": "text"
      },
      "source": [
        "Same above function can be achieved through a simple lambda function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6767ZK4ZOAhd",
        "colab_type": "code",
        "outputId": "9c07709f-9b27-4ec8-c2bb-4dfc6de68c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df['word_count'] = df['sentence'].apply(lambda x: len(str(x).split(\" \")))\n",
        "df.head(5)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>source</th>\n",
              "      <th>word_count_function</th>\n",
              "      <th>word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...  word_count\n",
              "0                           Wow... Loved this place.  ...           4\n",
              "1                                 Crust is not good.  ...           4\n",
              "2          Not tasty and the texture was just nasty.  ...           8\n",
              "3  Stopped by during the late May bank holiday of...  ...          15\n",
              "4  The selection on the menu was great and so wer...  ...          12\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RYDf5StaS5s",
        "colab_type": "text"
      },
      "source": [
        "Similarly, count the number of characters of each sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrT9GLxt5RgZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f643ba82-b1e6-453f-dc03-dd3584b4e028"
      },
      "source": [
        "df['char_count'] = df['sentence'].str.len()  # Includes the spaces\n",
        "df.head(5)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>source</th>\n",
              "      <th>word_count_function</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...  char_count\n",
              "0                           Wow... Loved this place.  ...          24\n",
              "1                                 Crust is not good.  ...          18\n",
              "2          Not tasty and the texture was just nasty.  ...          41\n",
              "3  Stopped by during the late May bank holiday of...  ...          87\n",
              "4  The selection on the menu was great and so wer...  ...          59\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmygJjvfabcd",
        "colab_type": "text"
      },
      "source": [
        "Calculate the average word length for each sentence.\n",
        "\n",
        "*   First, construct a method (avg_word()) which takes a sentence, split the sentence to words, then calculate the average word length.\n",
        "*   Using pandas dataframe apply() function and avg_word() method, calculate the average word length\n",
        "*  Assign the value to new column names 'avg_word'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-FyuyvnbMS3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0218c9a5-d031-47ed-8fad-6c135f54eaec"
      },
      "source": [
        "def avg_word(sentence):\n",
        "  words = sentence.split() # split the sentence into words\n",
        "  avg_of_words = (sum(len(word) for word in words)/len(words))\n",
        "  return avg_of_words\n",
        "\n",
        "df['avg_word'] = df['sentence'].apply(avg_word)\n",
        "df.head(5)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>source</th>\n",
              "      <th>word_count_function</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>avg_word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>5.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>3.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>41</td>\n",
              "      <td>4.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>87</td>\n",
              "      <td>4.866667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>59</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...  avg_word\n",
              "0                           Wow... Loved this place.  ...  5.250000\n",
              "1                                 Crust is not good.  ...  3.750000\n",
              "2          Not tasty and the texture was just nasty.  ...  4.250000\n",
              "3  Stopped by during the late May bank holiday of...  ...  4.866667\n",
              "4  The selection on the menu was great and so wer...  ...  4.000000\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQDXgq-V5pzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7NajYVd8t-r",
        "colab_type": "text"
      },
      "source": [
        "## Text pre-processing\n",
        "\n",
        "Pre-processing is mandatory for most text analytics tasks, as text in its raw format is unstructured and noisy. \n",
        "\n",
        "In the following snippets you will run several pre-processing steps.  \n",
        "\n",
        "**Please note that pre-processing is to be used with clear understanding of the expected outcome of text analytics, as each pre-processing step is not relevant or applicable to every NLP task.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ56xCM0dxLy",
        "colab_type": "text"
      },
      "source": [
        "Uppercase and lowercase characters are used for clarity in human communication. However, for a machine such distinction would create unnecessary complexities. Therefore, we transform all characters to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gowXmkYk50h_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cc690ac7-9e69-4229-d9c0-41dbc641bcf4"
      },
      "source": [
        "df['sentence'] = df['sentence'].str.lower()\n",
        "df.head()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>source</th>\n",
              "      <th>word_count_function</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>avg_word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wow... loved this place.</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>5.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>crust is not good.</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>3.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>41</td>\n",
              "      <td>4.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stopped by during the late may bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>87</td>\n",
              "      <td>4.866667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>59</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...  avg_word\n",
              "0                           wow... loved this place.  ...  5.250000\n",
              "1                                 crust is not good.  ...  3.750000\n",
              "2          not tasty and the texture was just nasty.  ...  4.250000\n",
              "3  stopped by during the late may bank holiday of...  ...  4.866667\n",
              "4  the selection on the menu was great and so wer...  ...  4.000000\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXvngUDDelRX",
        "colab_type": "text"
      },
      "source": [
        "Same with punctuation marks, we remove all using [regular expressions](https://www.w3schools.com/python/python_regex.asp) .  \n",
        "**Regular Expressions** - A regular expression is a special sequence of characters that helps you match or find other strings or sets of strings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHsiGbbY9Hqg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "084b953d-43a0-4d6b-ef20-93053b92ad4f"
      },
      "source": [
        "# This regular expression only keeps words and characters\n",
        "df['sentence'] = df['sentence'].str.replace('[^\\w\\s]','')\n",
        "df.head()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>source</th>\n",
              "      <th>word_count_function</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>avg_word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wow loved this place</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>5.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>crust is not good</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>3.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>not tasty and the texture was just nasty</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>41</td>\n",
              "      <td>4.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stopped by during the late may bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>87</td>\n",
              "      <td>4.866667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>59</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...  avg_word\n",
              "0                               wow loved this place  ...  5.250000\n",
              "1                                  crust is not good  ...  3.750000\n",
              "2           not tasty and the texture was just nasty  ...  4.250000\n",
              "3  stopped by during the late may bank holiday of...  ...  4.866667\n",
              "4  the selection on the menu was great and so wer...  ...  4.000000\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbhTrjC6lD1D",
        "colab_type": "text"
      },
      "source": [
        "### Remove digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-PafqgZe7x7",
        "colab_type": "text"
      },
      "source": [
        "For a sentiment analytics task, numbers or digits are not needed. Thus, we remove digits from the text dataset. \n",
        "\n",
        "However, for other tasks, numbers may be needed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKRv9btDlLNz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2ace895d-2fe2-4ebf-92c3-5556c0bd2343"
      },
      "source": [
        "def remove_digits(sent):\n",
        "  return \" \".join(w for w in sent.split() if not w.isdigit())\n",
        "\n",
        "df['sentence'] = df['sentence'].apply(remove_digits)\n",
        "df.head()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>source</th>\n",
              "      <th>word_count_function</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>avg_word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wow loved this place</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>5.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>crust is not good</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>3.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>not tasty and the texture was just nasty</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>41</td>\n",
              "      <td>4.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stopped by during the late may bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>87</td>\n",
              "      <td>4.866667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>59</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...  avg_word\n",
              "0                               wow loved this place  ...  5.250000\n",
              "1                                  crust is not good  ...  3.750000\n",
              "2           not tasty and the texture was just nasty  ...  4.250000\n",
              "3  stopped by during the late may bank holiday of...  ...  4.866667\n",
              "4  the selection on the menu was great and so wer...  ...  4.000000\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_xppk798hrz",
        "colab_type": "text"
      },
      "source": [
        "Demonstrate of the remove digit function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntqhwksm8T9M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a641b04-edc1-41f4-ec39-45ccd88ad63b"
      },
      "source": [
        "sample_text = 'Covid 19 is spreading fast'\n",
        "print(remove_digits(sample_text))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Covid is spreading fast\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlxTKunt8lyT",
        "colab_type": "text"
      },
      "source": [
        "What does \"\".join() means?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2kfwcVk8oyi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ccfd5ef-94ed-4f30-93b1-48a639f51869"
      },
      "source": [
        "word_list = [\"Covid\", \"is\", \"spreading\", \"fast\"]\n",
        "sentence = \"+\".join(word_list)\n",
        "print(sentence)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Covid+is+spreading+fast\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwh1imSm-BC3",
        "colab_type": "text"
      },
      "source": [
        "### Remove Stopwords\n",
        "\n",
        "[Stopwords](https://en.wikipedia.org/wiki/Stop_words) are deemed irrelevant for NLP purposes because they occur frequently in the language. Therefore, we will omit the stopwords as a pre-processing step. For this, we will use [NLTK](https://www.nltk.org/) library here.\n",
        "\n",
        "**NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) specifically for the English language written in  Python.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1J6fC9491k7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9a89f245-0871-4de8-d6bd-ba42e0b7ccb0"
      },
      "source": [
        "# Load NLTK library\n",
        "import nltk\n",
        "\n",
        "# Download the stopwords to the nltk library\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the stopwords\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecLfjQgmgYpk",
        "colab_type": "text"
      },
      "source": [
        "Have a look at the stopwords indexed in the NLTK library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkpvQNV5-N2g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5e937f70-93a9-409f-de8a-4ddda930cca5"
      },
      "source": [
        "stop = stopwords.words('english')\n",
        "print(stop)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPugCcb3gzMj",
        "colab_type": "text"
      },
      "source": [
        "Remove stopwords from the sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_HwAnis-Ut1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "78a421ae-87a3-46d6-e3ad-17d13e96ad9d"
      },
      "source": [
        "df['sentence'] = df['sentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "df.head(5)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>source</th>\n",
              "      <th>word_count_function</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>avg_word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wow loved place</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>5.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>crust good</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>3.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tasty texture nasty</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>41</td>\n",
              "      <td>4.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stopped late may bank holiday rick steve recom...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>87</td>\n",
              "      <td>4.866667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>selection menu great prices</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>59</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...  avg_word\n",
              "0                                    wow loved place  ...  5.250000\n",
              "1                                         crust good  ...  3.750000\n",
              "2                                tasty texture nasty  ...  4.250000\n",
              "3  stopped late may bank holiday rick steve recom...  ...  4.866667\n",
              "4                        selection menu great prices  ...  4.000000\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUol6pfo_MEL",
        "colab_type": "text"
      },
      "source": [
        "### Common and rare word analysis\n",
        "\n",
        "Aside from stopwords, some words appear rarely (only once or twice) in an entire body of text. \n",
        "Based on the analytics requirement, you can decide whether to keep or remove, and at what intensity/scale to remove."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iyA7bBDg_hB",
        "colab_type": "text"
      },
      "source": [
        "In order to do this, first we have to construct a word frequency dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHCEJC3V-wG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_frequency = pd.Series(' '.join(df['sentence']).split()).value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tsu7TJOhGGK",
        "colab_type": "text"
      },
      "source": [
        "List the top 10 common words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP6ids9d_ldS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5098d9c0-22dc-4f66-e087-9c046ed7612a"
      },
      "source": [
        "# Top common words\n",
        "word_frequency[:10]  # get top 10"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "good     225\n",
              "great    207\n",
              "movie    177\n",
              "phone    162\n",
              "film     155\n",
              "one      143\n",
              "like     123\n",
              "food     123\n",
              "place    114\n",
              "time     110\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEIsBYCGhLlY",
        "colab_type": "text"
      },
      "source": [
        "List the top 10 rare words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvUQt5hRAM13",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "60c007a9-4465-4302-b7f7-b3623dcd08da"
      },
      "source": [
        "# least common words\n",
        "word_frequency[-10:]  # get top 10"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "applause      1\n",
              "warn          1\n",
              "kindle        1\n",
              "sit           1\n",
              "foods         1\n",
              "reversible    1\n",
              "concerns      1\n",
              "mail          1\n",
              "george        1\n",
              "writers       1\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7hd1zK8AXSl",
        "colab_type": "text"
      },
      "source": [
        "### Spelling correction\n",
        "\n",
        "To correct misspelt words, we will use [textblob library](https://textblob.readthedocs.io/en/dev/) library. Keep in mind that corrections are always bound by the dictionary that you would use, and it may not account for context (their vs there).\n",
        "\n",
        "Due to the time complexity of spell-checking an entire corpus, in this exercise, we will use spell-check for just one example. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH_MOO58iity",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf9rFrJpARWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do not run this line of code.\n",
        "# Following line of code will correct spellings of all the sentences in the dataset.\n",
        "# df['sentence'] = df['sentence'].apply(lambda x: str(TextBlob(x).correct()))   # This will take a long time. Thus, we will show an seperate example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6ERDidUivTx",
        "colab_type": "text"
      },
      "source": [
        "Spelling correction example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6D1dM55A_5l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bcf8a5fd-5d7c-4bd8-88ac-dceab00dcce3"
      },
      "source": [
        "incorrect_text = 'bisness anlytis is an imptant skil seit for any organizaton'\n",
        "\n",
        "func = lambda x: str(TextBlob(x).correct())\n",
        "\n",
        "print(incorrect_text)\n",
        "print(str(TextBlob(incorrect_text).correct()))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bisness anlytis is an imptant skil seit for any organizaton\n",
            "business analysis is an important skin set for any organization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F_w641sCItw",
        "colab_type": "text"
      },
      "source": [
        "### Stemming\n",
        "\n",
        "[Stemming](https://en.wikipedia.org/wiki/Stemming) is the removal of prefix, suffix etc, to derive the base form of a word. We will use the NLTK library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MxoI8j4Bf93",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bbdf0dbd-b25b-418a-c1a9-501047a8cc7e"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stemming_function(sent):\n",
        "  word_list = sent.split()\n",
        "  stemmed_word_list = [stemmer.stem(word) for word in word_list]\n",
        "  stemmed_sentence = \" \".join(stemmed_word_list)\n",
        "  return stemmed_sentence\n",
        "\n",
        "df['sentence_stemmed'] = df['sentence'].apply(stemming_function)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>source</th>\n",
              "      <th>word_count_function</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>avg_word</th>\n",
              "      <th>sentence_stemmed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wow loved place</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>5.250000</td>\n",
              "      <td>wow love place</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>crust good</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>crust good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tasty texture nasty</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>41</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>tasti textur nasti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stopped late may bank holiday rick steve recom...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>87</td>\n",
              "      <td>4.866667</td>\n",
              "      <td>stop late may bank holiday rick steve recommen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>selection menu great prices</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>59</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>select menu great price</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...                                   sentence_stemmed\n",
              "0                                    wow loved place  ...                                     wow love place\n",
              "1                                         crust good  ...                                         crust good\n",
              "2                                tasty texture nasty  ...                                 tasti textur nasti\n",
              "3  stopped late may bank holiday rick steve recom...  ...  stop late may bank holiday rick steve recommen...\n",
              "4                        selection menu great prices  ...                            select menu great price\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuZ8AVKvDKQG",
        "colab_type": "text"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "[Lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html), unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.  \n",
        "  \n",
        "We will use  Wordnet for the lemmatization. Thus, we need to download Wordnet to the nltk library.\n",
        "\n",
        "WordNet is a lexical database for the English language. It groups English words into sets of synonyms called synsets, provides short definitions and usage\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8qJOjLwDhzB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ab560479-89c8-49ec-86dc-37a827c56ed2"
      },
      "source": [
        "# Download wordnet\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjCuPX1zjrWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmtizer = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRdYHVNQCiB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemmatize_function(sent):\n",
        "  word_list = sent.split()\n",
        "  lemma_word_list = [lemmtizer.lemmatize(word) for word in word_list]\n",
        "  lemma_sentence = \" \".join(lemma_word_list)\n",
        "  return lemma_sentence\n",
        "\n",
        "df['sentence_lemmatized'] = df['sentence'].apply(lemmatize_function)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKCDAK0PkCeW",
        "colab_type": "text"
      },
      "source": [
        "Display original pre-processed sentence, stemmed sentence and lemmatized sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbfKn2OnkDgl",
        "colab_type": "code",
        "outputId": "4e4b3e5a-16e6-4025-b011-db312cd4f5cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "df[['sentence', 'sentence_stemmed', 'sentence_lemmatized']].head(10)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentence_stemmed</th>\n",
              "      <th>sentence_lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wow loved place</td>\n",
              "      <td>wow love place</td>\n",
              "      <td>wow loved place</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>crust good</td>\n",
              "      <td>crust good</td>\n",
              "      <td>crust good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tasty texture nasty</td>\n",
              "      <td>tasti textur nasti</td>\n",
              "      <td>tasty texture nasty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stopped late may bank holiday rick steve recom...</td>\n",
              "      <td>stop late may bank holiday rick steve recommen...</td>\n",
              "      <td>stopped late may bank holiday rick steve recom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>selection menu great prices</td>\n",
              "      <td>select menu great price</td>\n",
              "      <td>selection menu great price</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>getting angry want damn pho</td>\n",
              "      <td>get angri want damn pho</td>\n",
              "      <td>getting angry want damn pho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>honeslty didnt taste fresh</td>\n",
              "      <td>honeslti didnt tast fresh</td>\n",
              "      <td>honeslty didnt taste fresh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>potatoes like rubber could tell made ahead tim...</td>\n",
              "      <td>potato like rubber could tell made ahead time ...</td>\n",
              "      <td>potato like rubber could tell made ahead time ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>fries great</td>\n",
              "      <td>fri great</td>\n",
              "      <td>fry great</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>great touch</td>\n",
              "      <td>great touch</td>\n",
              "      <td>great touch</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...                                sentence_lemmatized\n",
              "0                                    wow loved place  ...                                    wow loved place\n",
              "1                                         crust good  ...                                         crust good\n",
              "2                                tasty texture nasty  ...                                tasty texture nasty\n",
              "3  stopped late may bank holiday rick steve recom...  ...  stopped late may bank holiday rick steve recom...\n",
              "4                        selection menu great prices  ...                         selection menu great price\n",
              "5                        getting angry want damn pho  ...                        getting angry want damn pho\n",
              "6                         honeslty didnt taste fresh  ...                         honeslty didnt taste fresh\n",
              "7  potatoes like rubber could tell made ahead tim...  ...  potato like rubber could tell made ahead time ...\n",
              "8                                        fries great  ...                                          fry great\n",
              "9                                        great touch  ...                                        great touch\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6RTwgzzwsJg",
        "colab_type": "text"
      },
      "source": [
        "Stemmed algorithm seems to be working better in this case when compared to the lemmatization. It is recommended to observe the results after pre-processing tasks to understand the performance of the third party libraries we are using in pre-processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILB7exVHEY1y",
        "colab_type": "text"
      },
      "source": [
        "## Text Feature Extraction\n",
        "\n",
        "In a numeric dataset (e.g., house price dataset, titanic survival dataset, dungaree dataset), we had numeric and categorical variables, which we transformed to numeric values for predictive analytics. Those numeric variables are called numeric features in the datasets. Similarly, for NLP, we need to derive features from text data in numerical format because machines can only understand numeric representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FlXXX-cfHZa",
        "colab_type": "text"
      },
      "source": [
        "### N-Grams\n",
        "\n",
        "An [n-gram](https://en.wikipedia.org/wiki/N-gram) is a contiguous sequence of n items from a given sample of text or speech. They are basically a set of co-occuring words within a given window. When computing the n-grams, the shift is one-step forward (although you can move X words forward in more advanced scenarios). For example, for the sentence \"The cow jumps over the moon\". If N=2 (known as bigrams), then the ngrams would be:\n",
        "* the cow\n",
        "* cow jumps\n",
        "* jumps over\n",
        "* over the\n",
        "* the moon\n",
        " \n",
        "We will use NLTK ngrams and word_tokenizer libraries for n-gram feature extraction.\n",
        "\n",
        "Note: Need to download punkt resource for nltk for work tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inAc6aR2lwEg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "95aefb5f-e76c-4a85-f38a-96acfa245313"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJYeiIrClzp7",
        "colab_type": "text"
      },
      "source": [
        "First we define the value for *n*, in n-gram representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb18qzswl-Bq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX6iMftbmDPO",
        "colab_type": "text"
      },
      "source": [
        "Following n_grams() method will take a sentence and construct a list of n-grams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKPsz4T_l_ZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def n_grams(text):\n",
        "  n_grams = ngrams(word_tokenize(text), n)\n",
        "  return [' '.join(grams) for grams in n_grams]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2EEAEptmPrn",
        "colab_type": "text"
      },
      "source": [
        "Derive n-grams (n=3) for our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj2gxPH7DY84",
        "colab_type": "code",
        "outputId": "a9665b3b-7d75-49c4-b07a-ec0e193e721b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "df['3_grams'] = df['sentence'].apply(lambda x: n_grams(x))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XniThjwmV-r",
        "colab_type": "text"
      },
      "source": [
        "Display original sentence and n-grams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQUU-YcdmNTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "55db4021-4f50-4c59-9594-4d2b501b1105"
      },
      "source": [
        "df[['sentence', '3_grams']].head(20)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>3_grams</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wow loved place</td>\n",
              "      <td>[wow loved place]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>crust good</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tasty texture nasty</td>\n",
              "      <td>[tasty texture nasty]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stopped late may bank holiday rick steve recom...</td>\n",
              "      <td>[stopped late may, late may bank, may bank hol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>selection menu great prices</td>\n",
              "      <td>[selection menu great, menu great prices]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>getting angry want damn pho</td>\n",
              "      <td>[getting angry want, angry want damn, want dam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>honeslty didnt taste fresh</td>\n",
              "      <td>[honeslty didnt taste, didnt taste fresh]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>potatoes like rubber could tell made ahead tim...</td>\n",
              "      <td>[potatoes like rubber, like rubber could, rubb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>fries great</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>great touch</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>service prompt</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>would go back</td>\n",
              "      <td>[would go back]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>cashier care ever say still ended wayyy overpr...</td>\n",
              "      <td>[cashier care ever, care ever say, ever say st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>tried cape cod ravoli chickenwith cranberrymmmm</td>\n",
              "      <td>[tried cape cod, cape cod ravoli, cod ravoli c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>disgusted pretty sure human hair</td>\n",
              "      <td>[disgusted pretty sure, pretty sure human, sur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>shocked signs indicate cash</td>\n",
              "      <td>[shocked signs indicate, signs indicate cash]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>highly recommended</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>waitress little slow service</td>\n",
              "      <td>[waitress little slow, little slow service]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>place worth time let alone vegas</td>\n",
              "      <td>[place worth time, worth time let, time let al...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>like</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             sentence                                            3_grams\n",
              "0                                     wow loved place                                  [wow loved place]\n",
              "1                                          crust good                                                 []\n",
              "2                                 tasty texture nasty                              [tasty texture nasty]\n",
              "3   stopped late may bank holiday rick steve recom...  [stopped late may, late may bank, may bank hol...\n",
              "4                         selection menu great prices          [selection menu great, menu great prices]\n",
              "5                         getting angry want damn pho  [getting angry want, angry want damn, want dam...\n",
              "6                          honeslty didnt taste fresh          [honeslty didnt taste, didnt taste fresh]\n",
              "7   potatoes like rubber could tell made ahead tim...  [potatoes like rubber, like rubber could, rubb...\n",
              "8                                         fries great                                                 []\n",
              "9                                         great touch                                                 []\n",
              "10                                     service prompt                                                 []\n",
              "11                                      would go back                                    [would go back]\n",
              "12  cashier care ever say still ended wayyy overpr...  [cashier care ever, care ever say, ever say st...\n",
              "13    tried cape cod ravoli chickenwith cranberrymmmm  [tried cape cod, cape cod ravoli, cod ravoli c...\n",
              "14                   disgusted pretty sure human hair  [disgusted pretty sure, pretty sure human, sur...\n",
              "15                        shocked signs indicate cash      [shocked signs indicate, signs indicate cash]\n",
              "16                                 highly recommended                                                 []\n",
              "17                       waitress little slow service        [waitress little slow, little slow service]\n",
              "18                   place worth time let alone vegas  [place worth time, worth time let, time let al...\n",
              "19                                               like                                                 []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RJBlyLjh0Zq",
        "colab_type": "text"
      },
      "source": [
        "Based on above results (e.g., record 16) you can see that if there are only 2 words, the 3-grams would result no n-grams.  \n",
        "Thus, you may try to derive n-grams with *n=2*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhlvf58tqq3C",
        "colab_type": "text"
      },
      "source": [
        "### Bag of words\n",
        "\n",
        "[Bag of words](https://machinelearningmastery.com/gentle-introduction-bag-words-model/) is a simple text feature extraction mechanism.   \n",
        "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
        "* A vocabulary of known words.  \n",
        "* A measure of the presence of known words.  \n",
        "\n",
        "We will use [CountVectorizer library](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) on sklearn for bag-of-words model creation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDsvQka_oJGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBazktNpoPpH",
        "colab_type": "text"
      },
      "source": [
        "You may refer to [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) API for detailed description about the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKuTRNfYmqV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1), analyzer = \"word\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frDRiREJokx8",
        "colab_type": "text"
      },
      "source": [
        "Transform lemmatized senteces into bag-of-words model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1NwVXryrEj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_bow = bow.fit_transform(df['sentence_lemmatized'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLyqMA4CoxhA",
        "colab_type": "text"
      },
      "source": [
        "The X_bow would result in a term-document matrix.  \n",
        "e.g., Output format:  (sentence_id, vocabulary_dictionary_id) count\n",
        "* sentence_id - sentence id in the dataframe\n",
        "* vocabulary_dictionary_id - id of the particular word in the bag of words model dictionary\n",
        "* count - count of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imw2JQaYrM73",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "72535dec-604c-453a-a055-12ed54d5aa7a"
      },
      "source": [
        "print(X_bow)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 986)\t1\n",
            "  (0, 497)\t1\n",
            "  (0, 634)\t1\n",
            "  (1, 369)\t1\n",
            "  (2, 855)\t1\n",
            "  (2, 555)\t1\n",
            "  (3, 497)\t1\n",
            "  (3, 822)\t1\n",
            "  (3, 517)\t1\n",
            "  (3, 816)\t1\n",
            "  (4, 757)\t1\n",
            "  (4, 529)\t1\n",
            "  (4, 373)\t1\n",
            "  (4, 668)\t1\n",
            "  (5, 360)\t1\n",
            "  (5, 940)\t1\n",
            "  (5, 189)\t1\n",
            "  (5, 626)\t1\n",
            "  (6, 212)\t1\n",
            "  (6, 852)\t1\n",
            "  (6, 343)\t1\n",
            "  (7, 661)\t1\n",
            "  (7, 479)\t1\n",
            "  (7, 173)\t1\n",
            "  (7, 857)\t1\n",
            "  :\t:\n",
            "  (2741, 972)\t1\n",
            "  (2741, 491)\t1\n",
            "  (2741, 824)\t1\n",
            "  (2742, 563)\t1\n",
            "  (2742, 938)\t1\n",
            "  (2742, 549)\t1\n",
            "  (2743, 370)\t1\n",
            "  (2743, 849)\t1\n",
            "  (2743, 950)\t1\n",
            "  (2743, 87)\t1\n",
            "  (2744, 913)\t1\n",
            "  (2744, 976)\t1\n",
            "  (2744, 493)\t1\n",
            "  (2744, 321)\t1\n",
            "  (2744, 747)\t1\n",
            "  (2744, 673)\t1\n",
            "  (2745, 975)\t1\n",
            "  (2745, 269)\t1\n",
            "  (2746, 56)\t1\n",
            "  (2747, 585)\t1\n",
            "  (2747, 415)\t1\n",
            "  (2747, 945)\t1\n",
            "  (2747, 544)\t1\n",
            "  (2747, 435)\t1\n",
            "  (2747, 436)\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Fxf1C1iPN3",
        "colab_type": "text"
      },
      "source": [
        "### Term Frequency - Inverse Document Frequecy (TF-IDF)\n",
        "\n",
        "[Term frequency–inverse document frequency](https://www.kdnuggets.com/2018/08/wtf-tf-idf.html), is a numerical statistic that is intended to reflect how important a word is to a document in a collection. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\n",
        "\n",
        "We will use [feature extraction module](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) of the sklearn library for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFR61d5ehktG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzNi0OmbqSs7",
        "colab_type": "text"
      },
      "source": [
        "Construct TF-IDF using the lemmatized senteces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leDbV3eVjLUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_idf = vectorizer.fit_transform(df['sentence_lemmatized'])  # as the text data, we will use lemmatized sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kX79IoAqbdb",
        "colab_type": "text"
      },
      "source": [
        "Display the list of all the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzBSDPTKktzH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0ad50b78-1252-416c-b944-bd5f1ccd970c"
      },
      "source": [
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['10oct', '15lb', '18th', '1980s', '20th', '20the', '2mp', '30', '34ths', '3o', '40min', '5of', '5year', '70', '700w', '80', '815pm', '8pm', '90', 'aailiyah', 'abandoned', 'abhor', 'ability', 'able', 'abound', 'abovepretty', 'abroad', 'absolute', 'absolutel', 'absolutely', 'absolutley', 'abstruse', 'abysmal', 'ac', 'academy', 'accent', 'accept', 'acceptable', 'access', 'accessable', 'accessible', 'accessing', 'accessory', 'accessoryone', 'accident', 'accidentally', 'acclaimed', 'accolade', 'accommodation', 'accomodate', 'accompanied', 'according', 'accountant', 'accurate', 'accurately', 'accused', 'ache', 'achievement', 'achille', 'ackerman', 'acknowledged', 'across', 'act', 'acted', 'acting', 'actingeven', 'actingwise', 'action', 'activate', 'activated', 'activesync', 'actor', 'actorsan', 'actress', 'actual', 'actually', 'ad', 'adam', 'adaptation', 'adapter', 'add', 'added', 'addition', 'additional', 'address', 'adhesive', 'admins', 'admiration', 'admitted', 'adorable', 'adorablehis', 'adorablethe', 'adrift', 'adventure', 'advertised', 'advise', 'aerial', 'aesthetically', 'affected', 'affleck', 'affordable', 'afraid', 'africa', 'afternoon', 'age', 'aggravating', 'ago', 'agree', 'agreed', 'ahead', 'aimless', 'air', 'aired', 'airline', 'airport', 'akasha', 'akin', 'ala', 'alarm', 'albondigas', 'alert', 'alexander', 'alike', 'allergy', 'allison', 'allot', 'allow', 'allowing', 'allows', 'allstar', 'almond', 'almost', 'alone', 'along', 'alongside', 'alot', 'already', 'also', 'although', 'aluminum', 'always', 'amateurish', 'amaze', 'amazed', 'amazing', 'amazingly', 'amazingrge', 'amazingstylized', 'amazon', 'ambiance', 'ambience', 'america', 'american', 'among', 'amount', 'amp', 'ample', 'amusing', 'anatomist', 'andddd', 'andor', 'angel', 'angela', 'angeles', 'angelina', 'angle', 'angry', 'anguish', 'angus', 'animal', 'animated', 'animation', 'anita', 'ann', 'anne', 'anniversary', 'annoying', 'another', 'answer', 'ant', 'antena', 'anthony', 'anticipated', 'antiglare', 'antithesis', 'anymore', 'anyone', 'anything', 'anythinga', 'anytime', 'anyway', 'anyways', 'anywhere', 'apart', 'apartment', 'apologize', 'apology', 'app', 'appalling', 'apparently', 'appealing', 'appearance', 'appears', 'appetite', 'appetizer', 'applauded', 'applause', 'apple', 'applifies', 'appointment', 'appreciate', 'appropriate', 'approval', 'apt', 'area', 'arent', 'arepas', 'argued', 'arguing', 'aria', 'armageddon', 'armand', 'armband', 'around', 'array', 'arrival', 'arrived', 'arrives', 'arriving', 'art', 'article', 'articulated', 'artiness', 'artist', 'artistic', 'artless', 'as', 'asia', 'aside', 'ask', 'asked', 'asking', 'asleep', 'aspect', 'assante', 'assaulted', 'assistant', 'assumed', 'assure', 'astonishingly', 'astronaut', 'ate', 'atleast', 'atmosphere', 'atmosphere1', 'atrocious', 'atrocity', 'att', 'attached', 'attack', 'attacked', 'attempt', 'attempted', 'attempting', 'attention', 'attentive', 'attitude', 'attractive', 'audience', 'audio', 'auju', 'aurvåg', 'austen', 'austere', 'authentic', 'author', 'auto', 'autoanswer', 'available', 'average', 'aversion', 'avocado', 'avoid', 'avoided', 'avoiding', 'award', 'awarded', 'away', 'awesome', 'awful', 'awkward', 'awkwardly', 'awsome', 'ayce', 'aye', 'az', 'baaaaaad', 'baba', 'babbling', 'babie', 'baby', 'babysitting', 'bachi', 'back', 'backdrop', 'backed', 'background', 'backlight', 'bacon', 'bad', 'badass', 'badly', 'badwellits', 'bagel', 'bailey', 'bakery', 'baklava', 'balance', 'balanced', 'ball', 'ballet', 'bamboo', 'banana', 'band', 'bank', 'bar', 'barcelona', 'bare', 'barely', 'bargain', 'barking', 'barney', 'barren', 'bartender', 'baseball', 'based', 'basement', 'basic', 'basically', 'bat', 'batch', 'bates', 'bathroom', 'batter', 'battery', 'baxendale', 'bay', 'bbq', 'be3', 'beall', 'bean', 'bear', 'beat', 'beateous', 'beautiful', 'beautifully', 'beauty', 'became', 'bechard', 'become', 'becomes', 'bed', 'beef', 'beensteppedinandtrackedeverywhere', 'beep', 'beeping', 'beer', 'beforei', 'began', 'begin', 'beginning', 'behind', 'behing', 'behold', 'bela', 'believable', 'believe', 'believed', 'bell', 'bellagio', 'bellucci', 'belly', 'belmondo', 'belowpar', 'belt', 'ben', 'bend', 'bennett', 'bergen', 'bertolucci', 'besides', 'best', 'bethe', 'better', 'betty', 'beware', 'beyond', 'bible', 'big', 'bigbudget', 'bigger', 'biggest', 'bill', 'billy', 'binge', 'biographical', 'bipolarity', 'bird', 'biscuit', 'bisque', 'bit', 'bitch', 'bitchy', 'bite', 'bitpim', 'black', 'blackberry', 'blacktop', 'blah', 'blake', 'blame', 'bland', 'blandest', 'blandly', 'blanket', 'blare', 'blatant', 'blew', 'blist', 'block', 'bloddy', 'blood', 'bloodiest', 'bloody', 'blow', 'blown', 'blue', 'blueant', 'bluegreenscreen', 'bluetoooth', 'bluetooth', 'bluetoothmotorola', 'bluetooths', 'blush', 'bmw', 'boast', 'bob', 'boba', 'bodes', 'body', 'bohemian', 'boiled', 'boiling', 'bold', 'bombardment', 'bond', 'bone', 'bonus', 'boob', 'boogeyman', 'book', 'booking', 'booksomethats', 'boost', 'boot', 'bop', 'border', 'bordered', 'borderline', 'bore', 'bored', 'boring', 'boringpointless', 'borrowed', 'bos', 'bose', 'bother', 'bothersome', 'bottom', 'bottowm', 'bouchon', 'bought', 'bougth', 'bowl', 'box', 'boy', 'boyfriend', 'boyle', 'brain', 'brainsucking', 'brand', 'brat', 'bread', 'break', 'breakage', 'breakfast', 'breakfastlunch', 'breaking', 'breeder', 'breeze', 'brevity', 'brian', 'brick', 'brief', 'brigand', 'bright', 'brilliance', 'brilliant', 'brilliantly', 'bring', 'brings', 'brisket', 'broad', 'broke', 'broken', 'brokeni', 'brooding', 'brother', 'brought', 'brownish', 'browser', 'browsing', 'brunch', 'bruschetta', 'brushfire', 'brutal', 'bt', 'bt250v', 'bt50', 'bubbling', 'buck', 'bud', 'buddy', 'budget', 'buffalo', 'buffet', 'bug', 'build', 'builder', 'building', 'buildup', 'built', 'buldogis', 'bulky', 'bullock', 'bully', 'bumper', 'bunch', 'burger', 'burned', 'burrittos', 'burton', 'bus', 'business', 'bussell', 'busy', 'butter', 'button', 'buy', 'buyer', 'buyerbe', 'buying', 'buyit', 'buzzing', 'bye', 'ca42', 'caballeros', 'cable', 'caesar', 'cafe', 'café', 'cailles', 'cake', 'cakeohhh', 'calamari', 'calendar', 'california', 'call', 'called', 'calligraphy', 'calling', 'came', 'camelback', 'cameo', 'camera', 'camerawork', 'camp', 'campy', 'canada', 'canal', 'cancan', 'cancellation', 'cancelling', 'candace', 'candle', 'cando', 'cannoli', 'cannot', 'cant', 'capability', 'capacity', 'cape', 'caper', 'captain', 'capture', 'captured', 'car', 'carbs', 'card', 'cardboard', 'cardellini', 'care', 'careful', 'caring', 'carlys', 'carol', 'carpaccio', 'carrell', 'carried', 'carrier', 'carry', 'cart', 'cartel', 'cartoon', 'case', 'cash', 'cashew', 'cashier', 'casing', 'casino', 'cassette', 'cast', 'casted', 'casting', 'cat', 'catching', 'catchy', 'caterpillar', 'caught', 'cause', 'caused', 'causing', 'cavier', 'cbr', 'cd', 'cease', 'celebration', 'celebrity', 'cell', 'cellphone', 'cellular', 'celluloid', 'cent', 'center', 'central', 'century', 'certain', 'certainly', 'cg', 'cgi', 'chai', 'chain', 'chalkboard', 'challenge', 'chance', 'chanceit', 'change', 'changing', 'channel', 'char', 'character', 'characterage', 'characterisation', 'charcoal', 'charge', 'charged', 'chargelife', 'charger', 'charging', 'charisma', 'charismafree', 'charismatic', 'charles', 'charlie', 'charm', 'charming', 'chase', 'chasing', 'cheap', 'cheaper', 'cheaply', 'cheapy', 'cheated', 'check', 'checked', 'checking', 'cheek', 'cheekbone', 'cheerfull', 'cheerless', 'cheese', 'cheeseburger', 'cheesecurds', 'cheesiness', 'cheesy', 'chef', 'chemistry', 'chewy', 'chick', 'chicken', 'chickenwith', 'child', 'childhood', 'childlike', 'childrens', 'chill', 'chilly', 'chimplike', 'china', 'chinese', 'chip', 'chipolte', 'chipotle', 'chocolate', 'chodorov', 'choice', 'choked', 'choose', 'chosen', 'chou', 'chow', 'christmas', 'christopher', 'church', 'cibo', 'cinema', 'cinematic', 'cinematographer', 'cinematography', 'cinematographyif', 'cingulair', 'cingular', 'cingularatt', 'circumstance', 'claimed', 'clarity', 'class', 'classic', 'classical', 'classy', 'classywarm', 'clean', 'clear', 'clearer', 'clearly', 'clever', 'clichés', 'click', 'client', 'cliff', 'climax', 'climbing', 'clip', 'clipping', 'clock', 'close', 'closed', 'closeup', 'clothes', 'club', 'clue', 'coach', 'coal', 'coastal', 'coaster', 'cocktail', 'coconut', 'cod', 'coffee', 'coherent', 'cold', 'colder', 'cole', 'colleague', 'collect', 'collective', 'college', 'color', 'colored', 'colorful', 'colour', 'columbo', 'combination', 'combo', 'come', 'comedic', 'comedy', 'comfort', 'comfortable', 'comfortably', 'comfortible', 'comforting', 'comical', 'coming', 'command', 'comment', 'commentary', 'commented', 'commercial', 'common', 'communicate', 'communication', 'community', 'commuter', 'companion', 'company', 'comparablypriced', 'compared', 'compelling', 'compete', 'competent', 'competitor', 'complain', 'complained', 'complaint', 'complete', 'completed', 'completely', 'complex', 'complexity', 'compliment', 'composed', 'composition', 'comprehensible', 'compromise', 'computer', 'con', 'concentrate', 'concept', 'conception', 'conceptually', 'concern', 'concerning', 'concert', 'conclusion', 'concrete', 'condescends', 'condiment', 'condition', 'confidence', 'configuration', 'confirm', 'conflict', 'confortable', 'confuses', 'confusing', 'connect', 'connected', 'connecting', 'connection', 'connery', 'connerys', 'connoisseur', 'conrad', 'consequence', 'consider', 'considerable', 'considered', 'considering', 'considers', 'consistent', 'consolation', 'constant', 'constantine', 'constantly', 'constructed', 'construction', 'consumer', 'contact', 'contacted', 'contacting', 'contain', 'contained', 'container', 'containing', 'contains', 'content', 'continually', 'continuation', 'continue', 'continues', 'continuity', 'continuously', 'contract', 'contrast', 'contributing', 'contributory', 'contrived', 'control', 'controversy', 'contstruct', 'convenient', 'convention', 'conversation', 'converter', 'convey', 'convince', 'convincing', 'convoluted', 'cook', 'cooked', 'cooking', 'cool', 'copier', 'coppola', 'copy', 'cord', 'corded', 'core', 'corn', 'corny', 'corporation', 'correct', 'correction', 'correctly', 'cost', 'costars', 'costcos', 'costume', 'cotta', 'cotton', 'could', 'couldnt', 'count', 'counter', 'counterfeit', 'couple', 'coupon', 'course', 'court', 'courteous', 'courtroom', 'cover', 'coverage', 'covered', 'cow', 'cowardice', 'cox', 'coziness', 'crab', 'crack', 'crackedi', 'crackle', 'cradle', 'crafted', 'cramming', 'cranberrymmmm', 'crap', 'crappy', 'crash', 'crashed', 'craving', 'crawfish', 'crawl', 'crayon', 'crayonpencil', 'crazy', 'creak', 'cream', 'creamy', 'create', 'created', 'creates', 'creative', 'creativity', 'creature', 'credible', 'credit', 'crema', 'crepe', 'crew', 'crisp', 'crispy', 'critic', 'critical', 'crocdodile', 'crocs', 'cross', 'crostini', 'crouton', 'crowd', 'crowdpleaserthis', 'crowes', 'cruel', 'cruise', 'crumby', 'crust', 'crusty', 'cry', 'crystal', 'crêpe', 'cuisine', 'cult', 'culture', 'cumbersome', 'current', 'currently', 'curry', 'curtain', 'curve', 'custer', 'customer', 'customize', 'cut', 'cute', 'cutebut', 'cutest', 'cutie', 'cutout', 'cutting', 'd807', 'd807wrongly', 'dad', 'daily', 'damage', 'damian', 'damn', 'dance', 'danceall', 'dancing', 'dangerous', 'dark', 'darn', 'darren', 'data', 'date', 'daughter', 'day', 'de', 'dead', 'deadly', 'deadpan', 'deaf', 'deal', 'dealing', 'dealt', 'death', 'debated', 'debbie', 'debit', 'debut', 'decade', 'decay', 'decent', 'decide', 'decided', 'decidely', 'decipher', 'decision', 'decor', 'decorated', 'dedicated', 'dedication', 'dee', 'deep', 'deeply', 'def', 'defeat', 'defect', 'defective', 'defensemen', 'deffinitely', 'definately', 'defined', 'definitely', 'definitly', 'degree', 'del', 'delay', 'delete', 'delicate', 'delicioso', 'delicious', 'deliciously', 'delight', 'delightful', 'delish', 'deliver', 'delivered', 'delivering', 'delivers', 'delivery', 'dennys', 'dependant', 'depending', 'depends', 'depicted', 'depicts', 'depressing', 'depth', 'derivative', 'describe', 'described', 'describes', 'describing', 'description', 'desert', 'deserved', 'deserves', 'deserving', 'design', 'designed', 'designer', 'desired', 'desperately', 'desperation', 'despicable', 'despised', 'despite', 'dessert', 'destination', 'destroy', 'destroying', 'detachable', 'detail', 'detailed', 'detailing', 'deuchebaggery', 'develop', 'development', 'device', 'devine', 'di', 'diabetic', 'dialing', 'dialog', 'dialogue', 'diaper', 'dickens', 'didnt', 'die', 'died', 'dieing', 'difference', 'different', 'difficult', 'dignity', 'dime', 'dine', 'dining', 'dinner', 'dipping', 'directed', 'directing', 'direction', 'directly', 'director', 'directorial', 'directtovideo', 'dirt', 'dirty', 'disagree', 'disapoinment', 'disapointing', 'disappoint', 'disappointed', 'disappointing', 'disappointment', 'disapppointment', 'disaster', 'disbelief', 'discarded', 'discomfort', 'disconnected', 'discount', 'discovering', 'disgrace', 'disgraceful', 'disgust', 'disgusted', 'disgusting', 'dish', 'dislike', 'disliked', 'disneypixars', 'disparate', 'dispenser', 'display', 'displeased', 'disposable', 'disrespected', 'dissapointed', 'dissapointing', 'distant', 'distinction', 'distorted', 'distract', 'distracting', 'distressed', 'disturbing', 'dit', 'diverse', 'diving', 'division', 'dna', 'do', 'docking', 'doctor', 'documentary', 'dodge', 'doesnt', 'dog', 'dollar', 'dominated', 'done', 'donlevy', 'dont', 'donut', 'doomed', 'door', 'dosent', 'double', 'doubt', 'douchey', 'dough', 'doughy', 'down', 'download', 'downloading', 'downright', 'downside', 'downtown', 'dozen', 'dr', 'dracula', 'draft', 'drag', 'drago', 'drain', 'drained', 'drama', 'dramatic', 'drastically', 'drawback', 'drawing', 'dream', 'dreamed', 'dreary', 'drenched', 'dressed', 'dressing', 'dribble', 'dried', 'driest', 'drift', 'drifting', 'drink', 'drinking', 'dripping', 'drive', 'driving', 'drivng', 'droid', 'drooling', 'drop', 'dropped', 'dropping', 'drunk', 'dry', 'dualpurpose', 'duck', 'dude', 'due', 'duet', 'duethe', 'dull', 'dumb', 'dumbest', 'duo', 'duper', 'durable', 'duris', 'dusted', 'dustin', 'dustpan', 'dvd', 'dwight', 'dying', 'dylan', 'dysfunctionhe', 'e2', 'e715', 'ear', 'earbud', 'earbuds', 'earbugs', 'eargels', 'earlier', 'early', 'earpad', 'earphone', 'earpiece', 'earset', 'earth', 'ease', 'easier', 'easily', 'easy', 'eat', 'eaten', 'eating', 'ebay', 'ebola', 'eccleston', 'echo', 'eclectic', 'ed', 'edge', 'edible', 'edinburgh', 'editing', 'edition', 'educational', 'edward', 'eel', 'eew', 'effect', 'effective', 'efficient', 'effort', 'egg', 'eggplant', 'egotism', 'eighth', 'eiko', 'either', 'el', 'elaborately', 'elderly', 'electronics', 'elegant', 'elegantly', 'element', 'elia', 'elk', 'eloquently', 'else', 'elsewhere', 'email', 'embarassing', 'embarrassed', 'embarrassing', 'embassy', 'embedded', 'emerge', 'emilio', 'emily', 'emoting', 'emotion', 'emotionally', 'emperor', 'employee', 'empowerment', 'emptiness', 'empty', 'en', 'enchanting', 'encourage', 'end', 'endall', 'endearing', 'ended', 'ending', 'endlessly', 'energetic', 'energy', 'engaging', 'engineered', 'english', 'enhanced', 'enjoy', 'enjoyable', 'enjoyed', 'enjoyment', 'enough', 'ensued', 'enter', 'enterprise', 'entertained', 'entertaining', 'entertainment', 'enthusiastic', 'entire', 'entirely', 'entrance', 'entree', 'env', 'episode', 'equally', 'equipment', 'equivalent', 'era', 'ergonomic', 'ericson', 'ericsson', 'errol', 'error', 'escalating', 'escapism', 'especially', 'essence', 'essentially', 'establish', 'established', 'establishment', 'estate', 'estevez', 'estevezs', 'etc', 'etcits', 'ethic', 'europe', 'european', 'evaluate', 'eve', 'even', 'evening', 'event', 'eventually', 'ever', 'every', 'everybody', 'everyday', 'everyone', 'everyones', 'everything', 'everywhere', 'evidently', 'evil', 'evinced', 'evokes', 'exactly', 'exaggerating', 'example', 'excalibur', 'exceeding', 'exceeds', 'excelent', 'excellent', 'excellentangel', 'excellently', 'excels', 'except', 'exceptional', 'exceptionally', 'excerpt', 'excessive', 'excessively', 'exchange', 'exchanged', 'excited', 'exciting', 'exclaim', 'excruciatingly', 'excrutiatingly', 'excuse', 'executed', 'exemplar', 'exercise', 'existential', 'existing', 'expanded', 'expansive', 'expect', 'expectation', 'expected', 'expecting', 'expensive', 'experience', 'experienced', 'experiencing', 'expert', 'expertconnisseur', 'explain', 'explains', 'explanation', 'exploit', 'exploration', 'explosion', 'expression', 'exquisite', 'extant', 'extended', 'extensive', 'exterior', 'external', 'extra', 'extraneous', 'extraordinary', 'extremely', 'eye', 'eyed', 'eyepleasing', 'fabulous', 'face', 'faceplate', 'facial', 'facing', 'fact', 'factbased', 'factor', 'factory', 'fail', 'failed', 'fails', 'fair', 'fairly', 'faithful', 'falafel', 'fall', 'falling', 'falsely', 'falwell', 'fame', 'famed', 'familiar', 'family', 'famous', 'fan', 'fanciful', 'fantastic', 'fantasy', 'far', 'farce', 'fare', 'fascinated', 'fascinating', 'fascination', 'fast', 'faster', 'fat', 'father', 'faultless', 'fausa', 'faux', 'fav', 'favor', 'favorite', 'favourite', 'fear', 'feature', 'fee', 'feel', 'feelgood', 'feeling', 'feisty', 'fell', 'fella', 'fellow', 'fellowes', 'felt', 'female', 'ferry', 'fest', 'fi', 'fiancé', 'field', 'fifteen', 'fifty', 'figure', 'figured', 'file', 'filet', 'fill', 'fillet', 'filling', 'film', 'filmed', 'filmiing', 'filmmaker', 'filmmaking', 'filmmostly', 'filmography', 'filmsomething', 'final', 'finale', 'finally', 'financial', 'find', 'fine', 'finest', 'finger', 'fingernail', 'finish', 'finished', 'fire', 'fireball', 'firehouse', 'first', 'firstperson', 'fish', 'fishnet', 'fisted', 'fit', 'five', 'fix', 'flag', 'flair', 'flake', 'flaming', 'flash', 'flashback', 'flat', 'flatlined', 'flavor', 'flavored', 'flavorful', 'flavorless', 'flavourful', 'flaw', 'flawed', 'flawless', 'flawlessly', 'fleshed', 'flick', 'flimsy', 'flip', 'flipphones', 'fliptop', 'flirting', 'floor', 'flop', 'floppy', 'florida', 'flowed', 'flower', 'fluffy', 'flush', 'fly', 'flying', 'flynn', 'fm', 'fo', 'focus', 'focused', 'fodder', 'folk', 'follow', 'followed', 'following', 'follows', 'fond', 'fondue', 'food', 'foodand', 'foodservice', 'fooled', 'foolish', 'foot', 'footage', 'football', 'force', 'forced', 'ford', 'foreign', 'foreigner', 'forever', 'forgery', 'forget', 'forgettable', 'forgetting', 'forgot', 'forgotten', 'form', 'format', 'former', 'fort', 'forth', 'forty', 'forward', 'forwarded', 'found', 'four', 'fourth', 'fox', 'foxx', 'fraction', 'france', 'francis', 'francisco', 'frankly', 'freaking', 'free', 'freedom', 'freeman', 'freeway', 'freeze', 'freezing', 'french', 'frenchman', 'frequently4', 'frequentyly', 'fresh', 'freshness', 'friday', 'fried', 'friend', 'friendly', 'friendship', 'frightening', 'frog', 'front', 'frontier', 'frost', 'frozen', 'fruit', 'frustrated', 'frustration', 'fry', 'ft', 'fucking', 'fulci', 'fulfilling', 'fulfills', 'full', 'fully', 'fumbling', 'fun', 'function', 'functional', 'functionality', 'fundamental', 'funniest', 'funny', 'funnyall', 'furthermore', 'future', 'fuzzy', 'fx', 'gadget', 'gain', 'gake', 'galley', 'gallon', 'game', 'ganoush', 'garage', 'garbage', 'garbled', 'garbo', 'garden', 'garfield', 'garlic', 'gas', 'gaudi', 'gave', 'gay', 'gc', 'geek', 'geeky', 'gel', 'gem', 'general', 'generally', 'generates', 'generic', 'generous', 'genius', 'genre', 'gentletouch', 'gently', 'genuine', 'genuinely', 'george', 'gerardo', 'gere', 'get', 'getting', 'ghibili', 'giallo', 'giant', 'gibberish', 'gifted', 'gimmick', 'giovanni', 'girl', 'girlfriend', 'girlfriendboyfriend', 'girolamo', 'give', 'given', 'giving', 'glad', 'glance', 'glass', 'glassesthe', 'gloriously', 'glove', 'gloveseverything', 'glued', 'gluten', 'go', 'goalie', 'goat', 'god', 'godfather', 'goesthe', 'going', 'gold', 'goldencrispy', 'gone', 'gonna', 'good', 'good4', 'good7', 'google', 'gooodd', 'gordon', 'gore', 'goremeister', 'gorman', 'gosh', 'got', 'goth', 'gotta', 'gotten', 'gourmet', 'government', 'grab', 'grace', 'grade', 'gradually', 'grainy', 'grandmother', 'granted', 'graphic', 'grasp', 'grate', 'gratitude', 'gratuity', 'grease', 'greasy', 'great', 'greater', 'greatespecially', 'greatest', 'greatness', 'greatno', 'greedy', 'greek', 'green', 'greenstreet', 'greeted', 'grew', 'grey', 'grill', 'grilled', 'grim', 'grime', 'gringo', 'grip', 'gripping', 'gristle', 'grocery', 'groove', 'gross', 'grossed', 'ground', 'groundbreaking', 'group', 'grow', 'grtting', 'guacamole', 'guard', 'guess', 'guest', 'guilt', 'gung', 'guy', 'gx2', 'gyro', 'h500', 'ha', 'hackneyed', 'hadnt', 'haggis', 'hair', 'hairsplitting', 'half', 'halfway', 'halibut', 'ham', 'hamburger', 'han', 'hand', 'handdrawn', 'handed', 'handheld', 'handle', 'handled', 'handling', 'handmade', 'handsdown', 'handset', 'handsfree', 'handy', 'hang', 'hank', 'hankering', 'hanky', 'happen', 'happened', 'happening', 'happens', 'happier', 'happiness', 'happy', 'hard', 'hardest', 'hardly', 'harris', 'hasnt', 'hat', 'hate', 'hated', 'hatred', 'haul', 'haunt', 'havent', 'havilland', 'hawaiian', 'hay', 'hayao', 'hayworth', 'hbo', 'he', 'head', 'headband', 'headoverheels', 'headphone', 'headset', 'healthy', 'hear', 'heard', 'hearing', 'heart', 'heartwarming', 'heat', 'heaven', 'heavyit', 'heche', 'heimer', 'heist', 'held', 'helen', 'hell', 'hella', 'hellish', 'hello', 'helm', 'help', 'helped', 'helpful', 'helping', 'hence', 'hendrikson', 'here', 'hereas', 'herewhat', 'hernandez', 'hero', 'heroine', 'heroism', 'hey', 'hi', 'hide', 'high', 'higher', 'highest', 'highlight', 'highlighted', 'highly', 'highquality', 'hilarious', 'hill', 'hilt', 'hinge', 'hip', 'hiro', 'history', 'hit', 'hitch', 'hitchcock', 'ho', 'hockey', 'hoffman', 'hold', 'holder', 'holding', 'hole', 'holiday', 'hollander', 'hollow', 'hollywood', 'holster', 'home', 'homemade', 'homework', 'honeslty', 'honest', 'honestly', 'honor', 'hook', 'hooked', 'hoot', 'hope', 'hoped', 'hopefully', 'hopeless', 'hoping', 'horrendous', 'horrendously', 'horrible', 'horrid', 'horrified', 'horror', 'horrorsuspense', 'horse', 'hospitality', 'host', 'hostess', 'hosting', 'hot', 'hottest', 'hour', 'hoursthe', 'house', 'howdy', 'howe', 'howell', 'however', 'howeverthe', 'hs850', 'huevos', 'huge', 'hugo', 'human', 'humanity', 'humiliated', 'hummh', 'humming', 'hummus', 'humor', 'humorous', 'humour', 'hunan', 'hundred', 'hungry', 'hurry', 'hurt', 'husband', 'huston', 'hut', 'hybrid', 'hype', 'hypocrisy', 'iam', 'ians', 'ice', 'iced', 'id', 'idea', 'ideal', 'idealogical', 'identified', 'identifies', 'identify', 'idiot', 'idiotic', 'idiotsavant', 'idyllic', 'ie', 'iffy', 'ignore', 'ignored', 'igo', 'ill', 'im', 'imac', 'image', 'imaginable', 'imagination', 'imaginative', 'imagine', 'imagined', 'imdb', 'imitation', 'immediately', 'impact', 'impeccable', 'imperial', 'implausible', 'important', 'impossible', 'impressed', 'impression', 'impressive', 'improper', 'improve', 'improved', 'improvement', 'improvisation', 'impulse', 'inappropriate', 'incendiary', 'inch', 'included', 'includes', 'including', 'incoming', 'incomprehensible', 'inconsiderate', 'inconsistency', 'inconspicuous', 'incorrectness', 'increase', 'incrediable', 'incredible', 'incredibly', 'indeed', 'indescribably', 'indian', 'indicate', 'indication', 'indictment', 'indie', 'individual', 'indoor', 'indoors', 'indulgent', 'industrial', 'industry', 'ineptly', 'inexcusable', 'inexpensive', 'inexperience', 'inexplicable', 'infatuated', 'inflate', 'inform', 'informative', 'infra', 'infuriating', 'ingredient', 'inhouse', 'initially', 'innocence', 'insane', 'insanely', 'insert', 'inside', 'insincere', 'insipid', 'insomniac', 'inspiration', 'inspired', 'inspiring', 'install', 'installed', 'instance', 'instant', 'instantly', 'instead', 'instruction', 'instrument', 'insulin', 'insult', 'insulted', 'intangible', 'integral', 'integrated', 'integration', 'intelligence', 'intelligent', 'intended', 'intense', 'intensity', 'intention', 'interacting', 'interest', 'interested', 'interesting', 'interface', 'interim', 'interior', 'intermittently', 'internet', 'internetto', 'interplay', 'interpretation', 'interview', 'intoning', 'intrigued', 'invented', 'inventive', 'investment', 'inviting', 'involved', 'involves', 'involving', 'iphone', 'ipod', 'iq', 'ir', 'irda', 'ireland', 'iriver', 'iron', 'ironically', 'ironman', 'ironside', 'irritating', 'ishioka', 'isnt', 'issue', 'italian', 'itbuy', 'itdefinitely', 'item', 'itfriendly', 'itll', 'itmy', 'ive', 'jabra', 'jabra350', 'jack', 'jaclyn', 'jalapeno', 'jamaican', 'james', 'jamie', 'japanese', 'jason', 'jawbone', 'jay', 'jealousy', 'jean', 'jeff', 'jenni', 'jennifer', 'jerk', 'jerky', 'jerry', 'jessica', 'jessice', 'jet', 'jewel', 'jiggle', 'jim', 'jimmy', 'job', 'joes', 'joeys', 'john', 'join', 'joint', 'joke', 'jonah', 'jones', 'journey', 'joy', 'joyce', 'juano', 'judge', 'judging', 'judith', 'judo', 'juice', 'juicehighy', 'julian', 'june', 'junk', 'junkyard', 'jury', 'justice', 'jutland', 'jx10', 'kabuki', 'kanalys', 'kathy', 'keep', 'keeping', 'keira', 'keith', 'kept', 'kevin', 'key', 'keyboard', 'keypad', 'khao', 'kid', 'kiddos', 'kidnapped', 'kieslowski', 'kill', 'killer', 'killing', 'kind', 'kinda', 'kindle', 'kirk', 'kit', 'kitchen', 'kitchy', 'knew', 'knightley', 'knock', 'knocked', 'know', 'known', 'koteasjack', 'kris', 'kristoffersen', 'krussel', 'kudos', 'l7c', 'la', 'labute', 'lack', 'lacked', 'lacking', 'lady', 'lame', 'lance', 'landline', 'landscape', 'lane', 'lange', 'lap', 'laptop', 'large', 'largely', 'larger', 'laselva', 'lassie', 'last', 'lasted', 'lasting', 'lastly', 'latch', 'latched', 'late', 'lately', 'later', 'latest', 'latifas', 'latin', 'latte', 'latterday', 'laugh', 'laughable', 'laughedkids', 'laughing', 'law', 'lawyer', 'layer', 'lazy', 'le', 'lead', 'leading', 'leaf', 'leak', 'leap', 'learn', 'learned', 'least', 'leather', 'leave', 'leaving', 'leeand', 'left', 'leftover', 'leg', 'legal', 'legendary', 'legit', 'lemon', 'length', 'leni', 'lense', 'leopard', 'lesser', 'lesserknown', 'lesson', 'lestat', 'let', 'letdown', 'letting', 'lettuce', 'level', 'lewis', 'lg', 'lid', 'lie', 'lieutenant', 'life', 'lifemy', 'lifeoh', 'lifetime', 'light', 'lighter', 'lighting', 'lightly', 'lightweight', 'like', 'liked', 'liking', 'lil', 'lilli', 'lilt', 'limitation', 'limited', 'linda', 'line', 'lineanother', 'linked', 'linking', 'linksys', 'lino', 'lion', 'list', 'listed', 'listener', 'listening', 'lit', 'literally', 'littered', 'little', 'live', 'lived', 'living', 'livingworking', 'load', 'lobster', 'local', 'located', 'location', 'lock', 'locked', 'loewenhielms', 'logic', 'logitech', 'london', 'loneliness', 'long', 'longer', 'longwearing', 'look', 'looked', 'looking', 'loop', 'loos', 'loose', 'loosely', 'lord', 'lordy', 'los', 'lose', 'losing', 'lost', 'lot', 'loud', 'louder', 'loudest', 'loudglad', 'loudly', 'loudspeaker', 'lousy', 'lovable', 'love', 'loved', 'lovely', 'lover', 'loving', 'low', 'lowbudget', 'lower', 'lowkey', 'lox', 'loyal', 'loyalty', 'lucio', 'luck', 'lucy', 'lugosi', 'luke', 'lukewarm', 'lunch', 'lust', 'luvs', 'lyric', 'mac', 'macarons', 'macbeth', 'machine', 'mad', 'made', 'madhouse', 'madison', 'magazine', 'magic', 'magical', 'magnetic', 'magnificent', 'mail', 'main', 'maine', 'mainly', 'maintain', 'maintaining', 'maintains', 'major', 'majority', 'make', 'maker', 'making', 'male', 'malebonding', 'mall', 'malta', 'man', 'managed', 'management', 'managementoh', 'manager', 'manages', 'mandalay', 'mango', 'manna', 'mansonites', 'manual', 'manufacturer', 'many', 'marble', 'march', 'margarita', 'maria', 'marine', 'marion', 'mark', 'market', 'marred', 'marriage', 'marrow', 'martin', 'martini', 'mary', 'masculine', 'masculinity', 'massive', 'master', 'masterful', 'masterpiece', 'match', 'material', 'matrix', 'matter', 'matthew', 'mature', 'max', 'may', 'maybe', 'mayo', 'mayowell', 'mchattie', 'mclaglen', 'meagre', 'meal', 'mean', 'meander', 'meaning', 'meant', 'meat', 'meatball', 'meatloaf', 'mebunch', 'mechanism', 'medical', 'mediocre', 'mediterranean', 'medium', 'meet', 'meeverything', 'mega', 'megapixels', 'meh', 'mein', 'meld', 'mellow', 'melodrama', 'melt', 'melted', 'melville', 'member', 'memorable', 'memorized', 'memory', 'menace', 'menacing', 'mention', 'mentioned', 'menu', 'mercy', 'mere', 'meredith', 'merit', 'mesmerising', 'mesquite', 'mess', 'message', 'messaging', 'metal', 'meteorite', 'methe', 'metro', 'mexican', 'mgm', 'mic', 'michael', 'mickey', 'microphone', 'microsoft', 'microsofts', 'mid', 'middle', 'middleaged', 'might', 'mighty', 'mile', 'military', 'milk', 'milkshake', 'min', 'mind', 'mindbendingly', 'mindblowing', 'mine', 'miner', 'mini', 'miniseries', 'miniusb', 'minor', 'minute', 'minutesmajor', 'mirage', 'mirrormask', 'miserable', 'miserably', 'mishima', 'misleading', 'misplace', 'miss', 'missed', 'missing', 'mistake', 'mixed', 'miyazaki', 'miyazakis', 'mmmm', 'mobile', 'mode', 'model', 'modern', 'modest', 'moist', 'mojitos', 'mollusk', 'mom', 'moment', 'momentum', 'money', 'monica', 'monkey', 'monolog', 'monotonous', 'monster', 'monstrous', 'month', 'monumental', 'mood', 'moral', 'morgan', 'morning', 'moron', 'mortified', 'mostly', 'mother', 'motivation', 'moto', 'motor', 'motorola', 'motorolas', 'mountain', 'mouse', 'mouth', 'mouthful', 'move', 'moved', 'movement', 'movie', 'moviegoing', 'movieit', 'movieits', 'moviemaking', 'moving', 'moz', 'mozzarella', 'mp3', 'mp3s', 'mst3k', 'much', 'muddled', 'muddy', 'muffin', 'muffled', 'multigrain', 'multiple', 'muppets', 'murder', 'murdered', 'murdering', 'murky', 'mushroom', 'music', 'musician', 'musicincluding', 'mussel', 'must', 'musthave', 'muststop', 'mute', 'mystifying', 'naan', 'nacho', 'nakedbilly', 'name', 'nan', 'nano', 'nargile', 'narration', 'narrative', 'nasty', 'national', 'nationality', 'native', 'natural', 'nature', 'naughty', 'navigate', 'nay', 'nc17', 'near', 'nearly', 'neat', 'necklace', 'need', 'needed', 'needle', 'needlessly', 'needshandsfree', 'negative', 'negatively', 'negligent', 'negulesco', 'neighborhood', 'neighbourgirl', 'neil', 'neither', 'nerve', 'nervous', 'net', 'netflix', 'network', 'never', 'nevertheless', 'nevsky', 'new', 'next', 'ngage', 'nice', 'nicely', 'nicer', 'nicest', 'nicolas', 'night', 'nightmare', 'nigiri', 'nimoy', 'nine', 'ninja', 'noble', 'nobody', 'nobu', 'noca', 'noir', 'noircrimedrama', 'noise', 'nokia', 'noncliche', 'noncustomer', 'none', 'nonetheless', 'nonexistent', 'nonfancy', 'nonlinear', 'nonresearched', 'nonsense', 'nonsequel', 'noodle', 'normal', 'normally', 'north', 'northern', 'nostalgia', 'notable', 'notch', 'note', 'noted', 'noteworthy', 'nothing', 'nothingi', 'notice', 'noticed', 'novella', 'nude', 'number', 'numerous', 'nun', 'nurse', 'nut', 'nutbag', 'nutshell', 'nyc', 'obliged', 'obvious', 'obviously', 'occasion', 'occasional', 'occasionally', 'occupied', 'occur', 'occurs', 'oconnor', 'odd', 'oem', 'offend', 'offensive', 'offer', 'offered', 'offering', 'official', 'officially', 'often', 'oh', 'ohsomature', 'oil', 'ok', 'okay', 'old', 'olde', 'older', 'oldfashioned', 'ole', 'olive', 'olivia', 'omelet', 'omg', 'omit', 'one', 'onedimensional', 'onethis', 'onid', 'onion', 'online', 'onlyi', 'onscreen', 'ooze', 'open', 'opened', 'opening', 'opera', 'operate', 'operates', 'operation', 'opinion', 'opportunity', 'opposed', 'optimal', 'option', 'ordeal', 'order', 'ordered', 'ordering', 'organizational', 'oriented', 'origin', 'original', 'originality', 'originally', 'ortolani', 'oscar', 'others', 'otherwise', 'otto', 'ought', 'outdoor', 'outgoing', 'outhe', 'outit', 'outlandish', 'outlet', 'outperform', 'outrageously', 'outshining', 'outside', 'outstanding', 'outta', 'outward', 'oven', 'overacting', 'overall', 'overcome', 'overcooked', 'overdue', 'overhaul', 'overhip', 'overly', 'overnight', 'overnite', 'overpriced', 'override', 'overt', 'overwhelm', 'overwhelmed', 'overwrought', 'owed', 'owl', 'owned', 'owner', 'ownerchef', 'owneryou', 'owning', 'owns', 'oyster', 'oyvey', 'pace', 'paced', 'pacing', 'pack', 'package', 'packaged', 'packed', 'pad', 'paid', 'pain', 'painful', 'painfully', 'paint', 'painted', 'pair', 'paired', 'pairing', 'palance', 'palate', 'pale', 'palm', 'palmtopcameracellphone', 'pan', 'pancake', 'pandering', 'panna', 'pant', 'paolo', 'pap', 'paper', 'par', 'paradise', 'parent', 'park', 'parker', 'part', 'partaking', 'particular', 'particularly', 'party', 'passed', 'passion', 'past', 'pasta', 'pastry', 'pat', 'patent', 'pathetic', 'patient', 'patio', 'patriotism', 'patron', 'patty', 'paul', 'pause', 'pay', 'paying', 'pc', 'pda', 'pea', 'peach', 'peachykeen', 'peaking', 'peanut', 'pear', 'pearl', 'pecan', 'peculiarity', 'pedestal', 'peeling', 'pen', 'penne', 'penny', 'people', 'pepper', 'pepperand', 'perabo', 'perfect', 'perfected', 'perfection', 'perfectly', 'performance', 'performed', 'performing', 'perhaps', 'period', 'periodically', 'perpared', 'perplexing', 'person', 'personable', 'personality', 'personally', 'peter', 'petrified', 'petty', 'pg', 'pg13', 'pgrated', 'phantasm', 'phenomenal', 'philadelphia', 'philippa', 'pho', 'phoenix', 'phone', 'phonebattery', 'phonemy', 'phones2', 'phonesmp3', 'phonethe', 'phony', 'photo', 'photograph', 'photography', 'photographycinematography', 'phrase', 'physical', 'piano', 'pic', 'picked', 'picture', 'piece', 'pied', 'pile', 'pillow', 'pine', 'pineapple', 'pink', 'pissd', 'pita', 'pitch', 'pitiful', 'pixel', 'pizza', 'place', 'placed', 'plain', 'plan', 'plane', 'planned', 'planning', 'plant', 'plantain', 'plantronics', 'plantronincs', 'plastic', 'plate', 'plater', 'platter', 'play', 'played', 'player', 'playing', 'pleasant', 'pleasantly', 'please', 'pleased', 'pleaser', 'pleasure', 'pleather', 'pledge', 'plenty', 'plethora', 'plmer', 'plot', 'plug', 'plugged', 'plus', 'pm', 'pneumatic', 'pocket', 'poet', 'poetry', 'poignant', 'point', 'pointillistic', 'pointless', 'poised', 'poisoning', 'poler', 'polite', 'political', 'politically', 'politics', 'ponyo', 'poop', 'poor', 'poorly', 'pop', 'popcorn', 'popular', 'pork', 'port', 'portable', 'portion', 'portrait', 'portrayal', 'portrayed', 'portraying', 'positive', 'possesed', 'possibility', 'possible', 'possibly', 'posted', 'postinos', 'postproduction', 'potato', 'potentially', 'potted', 'poured', 'powdered', 'power', 'powerful', 'powerhouse', 'practical', 'practically', 'practice', 'prays', 'precisely', 'predict', 'predictable', 'predictably', 'prefer', 'preferably', 'prejudice', 'prelude', 'premise', 'premium', 'prepare', 'prepared', 'preparing', 'presence', 'present', 'presentation', 'president', 'pretentious', 'pretext', 'prettier', 'pretty', 'prettyoff', 'prevents', 'previous', 'price', 'priced', 'pricey', 'pricing', 'primal', 'primary', 'prime', 'print', 'privileged', 'pro', 'probably', 'problem', 'problemsthe', 'problemvery', 'procedure', 'proceeding', 'process', 'proclaimed', 'produce', 'produced', 'producer', 'product', 'production', 'professional', 'professor', 'profiterole', 'profound', 'program', 'progress', 'promise', 'promised', 'promote', 'prompt', 'prompted', 'promptly', 'prone', 'propaganda', 'properly', 'prosgood', 'protected', 'protection', 'protective', 'protector', 'protects', 'proud', 'proudly', 'proven', 'provide', 'provided', 'provides', 'providing', 'provokes', 'provoking', 'ps3', 'pseudosatanic', 'psyched', 'psychological', 'psychotic', 'pub', 'public', 'publicly', 'puck', 'puff', 'pull', 'pulled', 'pulling', 'pumpkin', 'punch', 'punched', 'punish', 'punishment', 'puppet', 'purcashed', 'purchase', 'purchased', 'purchasing', 'pure', 'puree', 'purity', 'purpose', 'puréed', 'push', 'pushed', 'put', 'putting', 'puzzlesolving', 'pyromaniac', 'quaid', 'quaint', 'qualified', 'quality', 'quantity', 'question', 'questioning', 'quick', 'quicker', 'quickly', 'quiet', 'quinn', 'quit', 'quite', 'québec', 'qwerty', 'race', 'racial', 'racism', 'radiant', 'raging', 'ramseys', 'ranch', 'rancheros', 'random', 'randomly', 'range', 'rank', 'rapidly', 'rare', 'rarely', 'raspberry', 'rate', 'rated', 'rather', 'rating', 'ratio', 'rave', 'raver', 'raving', 'ravoli', 'raw', 'ray', 'razor', 'razr', 'reach', 'reaching', 'reaction', 'read', 'reader', 'reading', 'ready', 'real', 'realised', 'realistic', 'reality', 'realize', 'realized', 'really', 'realworld', 'reason', 'reasonable', 'reasonably', 'rebootsoverall', 'recall', 'reccomendation', 'reccommend', 'receipt', 'receive', 'received', 'receives', 'receiving', 'recent', 'recently', 'reception', 'receptiona', 'receptionsound', 'recessed', 'recharge', 'recieve', 'recognition', 'recognizes', 'recommend', 'recommendation', 'recommended', 'recommending', 'reconciliation', 'recover', 'recurring', 'red', 'redeemed', 'redeeming', 'reduction', 'reenactment', 'reference', 'refill', 'reflected', 'refrained', 'refreshing', 'refried', 'refund', 'refurb', 'refuse', 'refused', 'regarding', 'regardless', 'register', 'regret', 'regrettable', 'regrettably', 'regretted', 'regular', 'regularly', 'reheated', 'rejection', 'relate', 'related', 'relation', 'relationship', 'relative', 'relatively', 'relax', 'relaxed', 'relaxing', 'release', 'released', 'reliability', 'relief', 'relleno', 'relocated', 'relying', 'remaining', 'remake', 'remarkable', 'remember', 'reminded', 'reminds', 'remorse', 'remotely', 'removing', 'render', 'rendering', 'rendition', 'renowned', 'rent', 'reoccurebottom', 'repair', 'repeat', 'repeated', 'repeating', 'repertory', 'replace', 'replaced', 'replaceeasy', 'replacement', 'replacementr', 'replenished', 'reporter', 'represents', 'requested', 'require', 'requirement', 'rescue', 'research', 'reservation', 'reset', 'resolution', 'resounding', 'respect', 'rest', 'restaraunt', 'restart', 'restaurant', 'restocking', 'restored', 'restrained', 'result', 'resume', 'retarded', 'retreat', 'return', 'returned', 'returning', 'revealing', 'revenge', 'revere', 'reverse', 'reversestereotypes', 'reversible', 'review', 'reviewer', 'reviewing', 'revisiting', 'ri', 'rib', 'ribeye', 'rice', 'rick', 'rickman', 'ride', 'ridiculous', 'ridiculousness', 'right', 'rightthe', 'riingtones', 'ring', 'ringer', 'ringing', 'ringtones', 'rinse', 'riot', 'rip', 'ripped', 'rise', 'risk', 'risotto', 'rita', 'rivalry', 'riveted', 'riz', 'road', 'roam', 'roast', 'roasted', 'robert', 'robotic', 'rochonwas', 'rock', 'rocked', 'rocketed', 'roeg', 'role', 'roll', 'rolled', 'romanticcharminghilariousand', 'room', 'roosevelt', 'rotating', 'roth', 'rough', 'round', 'routine', 'row', 'rowdy', 'rpg', 'rpger', 'rubber', 'rubberpetroleum', 'rubbish', 'rubin', 'rude', 'rudely', 'rumble', 'run', 'running', 'rushed', 'ruthless', 'ryan', 'ryans', 's11', 's710a', 'sabotage', 'sack', 'sad', 'sadly', 'saffron', 'saganaki', 'saggy', 'said', 'sake', 'salad', 'salesman', 'salmon', 'sals', 'salsa', 'salt', 'salty', 'sam', 'sample', 'samsung', 'samsungcrap', 'san', 'sand', 'sandra', 'sandwich', 'sangria', 'sanyo', 'sappiest', 'sarcophage', 'sashimi', 'sat', 'satifying', 'satisfied', 'satisfying', 'satisifed', 'sauce', 'sause', 'savalas', 'save', 'saved', 'saving', 'savor', 'saw', 'say', 'saying', 'scale', 'scallop', 'scamp', 'scare', 'scared', 'scaredand', 'scary', 'scene', 'scenery', 'schilling', 'schizophrenic', 'school', 'schoolers', 'schr450', 'schrader', 'schultz', 'sci', 'science', 'scientist', 'scifis', 'score', 'scot', 'scottsdale', 'scratch', 'scratched', 'scream', 'screamy', 'screen', 'screened', 'screenjames', 'screenplay', 'screenthis', 'screenwriter', 'screwed', 'scrimm', 'script', 'scripted', 'scripting', 'sculpture', 'sea', 'seafood', 'seal', 'seamless', 'seamlessly', 'sean', 'searched', 'season', 'seasonal', 'seasoned', 'seasoning', 'seat', 'seated', 'seating', 'second', 'secondary', 'secondly', 'section', 'secure', 'securely', 'securly', 'see', 'seeen', 'seeing', 'seem', 'seemed', 'seems', 'seen', 'selection', 'self', 'selfdiscovery', 'selfindulgent', 'selfpreservation', 'selfrespecting', 'selfsacrifice', 'sell', 'seller', 'semi', 'send', 'sending', 'senior', 'sens', 'sense', 'sensibility', 'sensitive', 'sensitivity', 'sensor', 'sent', 'sentiment', 'seperate', 'seperated', 'sequel', 'sequence', 'serf', 'sergeant', 'series', 'serious', 'seriously', 'serivce', 'serve', 'served', 'server', 'service', 'servicecheck', 'serving', 'set', 'setting', 'setup', 'seuss', 'sever', 'several', 'severe', 'sewer', 'sex', 'sexobsessed', 'sexy', 'shakespear', 'shakespears', 'shall', 'shallow', 'shame', 'shameful', 'shape', 'share', 'sharing', 'sharp', 'sharply', 'shatner', 'shattered', 'shawarrrrrrma', 'shed', 'sheer', 'shelf', 'shell', 'shenanigan', 'shepard', 'shes', 'shield', 'shifting', 'shine', 'shined', 'shiny', 'shipment', 'shipped', 'shipping', 'shirley', 'shirt', 'shocked', 'shocking', 'shoe', 'shoot', 'shooter', 'shooting', 'shop', 'shopping', 'short', 'shortlist', 'shot', 'shouldnt', 'shouldve', 'shouting', 'show', 'showcasing', 'showed', 'shower', 'showthese', 'shrimp', 'shutdown', 'sibling', 'sick', 'side', 'sidelined', 'sight', 'sign', 'signal', 'significant', 'significantly', 'silent', 'silently', 'silly', 'sim', 'similar', 'similarly', 'simmering', 'simple', 'simpler', 'simplifying', 'simply', 'sin', 'since', 'sincere', 'sing', 'singing', 'single', 'sinister', 'sink', 'sinking', 'sister', 'sit', 'sitcom', 'sitdown', 'site', 'sits', 'sitting', 'situation', 'situations1', 'six', 'size', 'sketchy', 'skilled', 'skimp', 'skip', 'skype', 'slacker', 'slavic', 'slaw', 'sleek', 'sleep', 'slice', 'sliced', 'slid', 'slide', 'slider', 'slideshow', 'sliding', 'slightest', 'slightly', 'slim', 'slimy', 'slipping', 'sloppy', 'slow', 'slowly', 'slowmotion', 'slowmoving', 'slur', 'smack', 'small', 'smaller', 'smallest', 'smart', 'smartphone', 'smashburger', 'smeared', 'smell', 'smelled', 'smile', 'smiling', 'smith', 'smoke', 'smoking', 'smooth', 'smoother', 'smoothly', 'smoothy', 'smudged', 'snap', 'snider', 'snow', 'snug', 'so', 'soap', 'sobaditsgood', 'sobaditsmemorable', 'sobering', 'social', 'soft', 'software', 'soggy', 'soi', 'sold', 'soldier', 'sole', 'solid', 'solidify', 'solidifying', 'somehow', 'someone', 'something', 'sometimes', 'somewhat', 'somewhere', 'son', 'song', 'sony', 'soon', 'sooner', 'soooo', 'sooooo', 'soooooo', 'sophisticated', 'sore', 'sorely', 'sorrentino', 'sorry', 'sort', 'soul', 'sound', 'sounded', 'soundtrack', 'soundwise', 'soup', 'sour', 'source', 'south', 'southern', 'southwest', 'soyo', 'space', 'spacek', 'spacey', 'spaghetti', 'span', 'speak', 'speaker', 'speakerphone', 'speaking', 'spec', 'special', 'specialand', 'specially', 'specialtoo', 'speed', 'speedy', 'spend', 'spends', 'spent', 'spew', 'sphere', 'spice', 'spicier', 'spicy', 'spiffy', 'spinach', 'spinn', 'splendid', 'spock', 'spoil', 'spoiled', 'spoiler', 'sporting', 'spot', 'spotty', 'spring', 'sprint', 'sprout', 'spy', 'squib', 'stable', 'staff', 'stage', 'stagey', 'stagy', 'stale', 'stand', 'standard', 'standout', 'stanwyck', 'stanwycks', 'star', 'stari', 'starlet', 'starring', 'start', 'startac', 'started', 'starter', 'starving', 'state', 'stated', 'stateoftheart', 'static', 'station', 'stay', 'stayed', 'staying', 'steak', 'steakhouse', 'stealing', 'steamboat', 'steele', 'steep', 'steer', 'steiner', 'step', 'stephen', 'stepped', 'stereo', 'stereotype', 'stereotypically', 'steve', 'stewart', 'stick', 'still', 'stink', 'stinker', 'stir', 'stocking', 'stoic', 'stomach', 'stood', 'stop', 'stopped', 'storage', 'store', 'storm', 'story', 'storyline', 'storytellinga', 'stowe', 'strange', 'stranger', 'strap', 'stratus', 'straw', 'strawberry', 'stream', 'street', 'strength', 'stress', 'stretch', 'strident', 'strike', 'string', 'strip', 'strives', 'stroke', 'strong', 'struck', 'structure', 'struggle', 'stuart', 'stuck', 'student', 'studio', 'study', 'stuff', 'stuffed', 'stunning', 'stupid', 'stupidity', 'sturdiness', 'sturdy', 'style', 'styling', 'stylish', 'styrofoam', 'sub', 'subgenre', 'subject', 'sublime', 'sublimely', 'submerged', 'subpar', 'subplot', 'subplots', 'subtitle', 'subtle', 'subversive', 'subverting', 'subway', 'succeeded', 'succeeds', 'success', 'succulent', 'suck', 'sucked', 'sucker', 'sudden', 'suddenly', 'suffered', 'suffering', 'suffers', 'sugar', 'sugary', 'suggest', 'suggestion', 'suggests', 'suited', 'sum', 'summarize', 'summary', 'summer', 'sun', 'sunday', 'sunglass', 'super', 'superb', 'superbad', 'superbly', 'superfast', 'superficial', 'superintelligent', 'superlative', 'supernatural', 'supertooth', 'support', 'supporting', 'supposed', 'supposedly', 'suprised', 'sure', 'surefire', 'surely', 'surf', 'surface', 'surprise', 'surprised', 'surprising', 'surprisingly', 'surrounding', 'surroundings', 'survived', 'survivor', 'sushi', 'suspense', 'suspension', 'sven', 'swamp', 'sweep', 'sweet', 'sweetest', 'switch', 'switched', 'swivel', 'sword', 'swung', 'sydney', 'sympathetic', 'sync', 'synchronization', 'syrupy', 'system', 'table', 'tacky', 'taco', 'taelons', 'tailored', 'take', 'taken', 'takeout', 'taking', 'tale', 'talent', 'talented', 'talk', 'talking', 'tank', 'tap', 'tapa', 'tape', 'taped', 'tardis', 'tartar', 'tartare', 'task', 'taste', 'tasted', 'tasteless', 'tasting', 'tasty', 'tater', 'taxidermist', 'taylor', 'tea', 'teach', 'teacher', 'team', 'teamwork', 'tear', 'tech', 'technically', 'technology', 'teddy', 'tedium', 'teen', 'teenager', 'teeth', 'telephone', 'television', 'tell', 'telly', 'temperament', 'tempo', 'ten', 'tender', 'tension', 'tepid', 'term', 'terminology', 'terrible', 'terribly', 'terrific', 'terror', 'texas', 'text', 'texture', 'th', 'thai', 'thank', 'thanks', 'thats', 'thatsucked', 'theater', 'theatre', 'theatrical', 'theft', 'theme', 'themeat', 'theory', 'therapy', 'there', 'thereplacement', 'theyd', 'theyre', 'thick', 'thin', 'thing', 'think', 'thinking', 'thinly', 'third', 'thirty', 'thiswhen', 'thomerson', 'thorn', 'thoroughly', 'thorsen', 'though', 'thought', 'thoughtprovoking', 'thoughtsgabriels', 'thousand', 'thread', 'three', 'threepack', 'threshold', 'threw', 'thrilled', 'thriller', 'thrillerhorror', 'throughout', 'throwback', 'thrown', 'thru', 'thug', 'thumb', 'thumper', 'thunderbird', 'thus', 'tick', 'ticker', 'ticket', 'ticking', 'tied', 'tigerlilly', 'tight', 'tightly', 'time', 'timeframe', 'timeless', 'timely', 'timer', 'timewaster', 'timing', 'tinny', 'tiny', 'tip', 'tiramisu', 'tired', 'title', 'titta', 'tmobile', 'toactivate', 'toast', 'toasted', 'today', 'together', 'togo', 'toilet', 'told', 'tolerable', 'tolerance', 'tolerate', 'tom', 'tomato', 'tomorrow', 'ton', 'tone', 'toneoverall', 'tongue', 'tonight', 'tony', 'took', 'tool', 'toon', 'tooth', 'top', 'topic', 'topvery', 'toro', 'torture', 'tortured', 'tot', 'total', 'totally', 'touch', 'touched', 'touching', 'tough', 'towards', 'tower', 'town', 'townsend', 'tracfone', 'tracfonewebsite', 'track', 'tracking', 'tract', 'traditional', 'traffic', 'tragedy', 'trailer', 'trainroller', 'tranquillity', 'transceiver', 'transcend', 'transcendant', 'transfer', 'transformed', 'translate', 'translating', 'transmission', 'transmit', 'transmitter', 'trap', 'trash', 'trashy', 'travled', 'treachery', 'treasure', 'treat', 'treated', 'treatment', 'trek', 'tremendous', 'tremendously', 'treo', 'tribute', 'tricky', 'tried', 'trilogy', 'trimmed', 'trinity', 'trip', 'trippy', 'triumphed', 'trond', 'trooper', 'trouble', 'truck', 'true', 'truffle', 'truly', 'trumbull', 'trumpeter', 'trunk', 'trust', 'truth', 'try', 'trying', 'tryst', 'trythe', 'tsunami', 'tucson', 'tummy', 'tuna', 'tuneful', 'tungsten', 'turkey', 'turn', 'turned', 'tv', 'tvnever', 'twice', 'twirling', 'twist', 'two', 'tying', 'type', 'typical', 'ue', 'ugliest', 'ugly', 'uhura', 'ultracheap', 'um', 'unacceptable', 'unacceptableunless', 'unacceptible', 'unaccompanied', 'unbearable', 'unbearably', 'unbelievable', 'unbelievably', 'uncalled', 'uncomfortable', 'unconditional', 'unconvincing', 'underacting', 'underappreciated', 'underbite', 'undercooked', 'underline', 'underlying', 'underneath', 'underrated', 'underservices', 'understand', 'understanding', 'understated', 'understatement', 'understood', 'undertone', 'underwater', 'underwhelming', 'undoubtedly', 'uneasy', 'unemployed', 'unethical', 'unexperienced', 'unfaithful', 'unfolds', 'unforgettable', 'unfortunate', 'unfortunately', 'unfunny', 'unhappy', 'unhealthy', 'uninspired', 'unintelligible', 'unintentionally', 'uninteresting', 'union', 'unique', 'uniqueness', 'unit', 'universal', 'universe', 'unknown', 'unless', 'unlike', 'unlockable', 'unmatched', 'unmitigated', 'unmoving', 'unnecessary', 'unneeded', 'unoriginal', 'unpleasant', 'unpredictability', 'unpredictable', 'unprofessional', 'unreal', 'unrealistic', 'unrecognizable', 'unrecommended', 'unreliable', 'unremarkable', 'unrestrained', 'unsatisfactory', 'unsatisfying', 'untoasted', 'unusable', 'unwatchable', 'unwelcome', 'unwrapped', 'upandcoming', 'upas', 'upbeat', 'update', 'updatewent', 'upgrade', 'upgrading', 'uplifting', 'upload', 'uploaded', 'upper', 'ups', 'upstairs', 'uptight', 'upway', 'ursula', 'us', 'usable', 'usage', 'usb', 'use', 'used', 'useful', 'usefulness', 'useless', 'user', 'using', 'ussr', 'usual', 'usually', 'utter', 'utterly', 'v115g', 'v265', 'v325i', 'v3c', 'v3i', 'vacant', 'vain', 'valentine', 'valley', 'value', 'vampire', 'vandiver', 'vanilla', 'variation', 'veal', 'vega', 'veganveggie', 'vegasthere', 'vegetable', 'vegetarian', 'veggitarian', 'vehicle', 'velvet', 'ventilation', 'ventura', 'venture', 'venturing', 'venue', 'verbal', 'verbatim', 'verge', 'verizon', 'verizons', 'versatile', 'version', 'versus', 'vessel', 'veteran', 'via', 'vibe', 'victor', 'video', 'view', 'viewer', 'viewing', 'villain', 'vinaigrette', 'vinegrette', 'violence', 'violinist', 'violinplaying', 'virgin', 'virtue', 'virus', 'vision', 'visit', 'visited', 'visor', 'visual', 'visually', 'vitally', 'vivian', 'vivid', 'vocal', 'vodka', 'voice', 'voiceovers', 'volatile', 'volcano', 'voltage', 'volume', 'vomit', 'vomited', 'voodoo', 'voted', 'voyage', 'vulcan', 'vx', 'vx9900', 'w810i', 'waaaaaayyyyyyyyyy', 'waaay', 'wagyu', 'wait', 'waited', 'waiter', 'waiting', 'waitress', 'wake', 'walk', 'walked', 'walkman', 'wall', 'wallet', 'want', 'wanted', 'wanting', 'war', 'warm', 'warmer', 'warmth', 'warn', 'warning', 'warranty', 'wart', 'wartime', 'wash', 'washed', 'washing', 'wasnt', 'waste', 'wasted', 'wasting', 'watch', 'watchable', 'watched', 'watching', 'water', 'watered', 'waterproof', 'watkins', 'watson', 'wave', 'way', 'waylaid', 'wayne', 'wayyy', 'wb', 'weak', 'weaker', 'wear', 'wearing', 'weaving', 'web', 'website', 'wed', 'wedding', 'wedge', 'week', 'weekend', 'weekly', 'weight', 'weird', 'welcome', 'well', 'wellbalanced', 'welldesigned', 'welldone', 'wellit', 'wellpaced', 'wellwell', 'welsh', 'went', 'werent', 'weve', 'whatever', 'whatsoever', 'whenever', 'whenscamp', 'whether', 'whine', 'whiny', 'whistle', 'white', 'who', 'whoa', 'whoever', 'whole', 'wholesome', 'whose', 'wide', 'widmark', 'wienerschnitzel', 'wife', 'wifetobe', 'wifi', 'wih', 'wild', 'wildly', 'wilkinson', 'william', 'willie', 'wily', 'win', 'wind', 'window', 'windresistant', 'wine', 'wing', 'winner', 'wiping', 'wire', 'wired', 'wirefly', 'wireless', 'wise', 'wish', 'wit', 'within', 'without', 'witnessed', 'witticism', 'witty', 'woa', 'wobbly', 'woman', 'wonder', 'wondered', 'wonderful', 'wonderfully', 'wong', 'wont', 'wonton', 'woo', 'wood', 'wooden', 'word', 'wordofmouth', 'work', 'worked', 'worker', 'working', 'workingeating', 'world', 'worldweariness', 'wornout', 'worry', 'worse', 'worst', 'worstannoying', 'worth', 'worthless', 'worthwhile', 'worthy', 'would', 'wouldbe', 'wouldnt', 'wouldve', 'wound', 'woven', 'wow', 'wrap', 'wrapped', 'write', 'writer', 'writing', 'written', 'wrong', 'wrongfirst', 'wrotedirected', 'yaall', 'yama', 'yardley', 'yawn', 'yay', 'yeah', 'year', 'yearsgreat', 'yell', 'yellow', 'yellowtail', 'yelp', 'yelpers', 'yes', 'yet', 'youd', 'youdo', 'youll', 'young', 'younger', 'youre', 'youthful', 'youtube', 'youve', 'yucky', 'yukon', 'yum', 'yummy', 'yun', 'z500a', 'zero', 'zillion', 'zombie', 'zombiestudents', 'zombiez']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8oekEhSlt4r",
        "colab_type": "text"
      },
      "source": [
        "Here you see there are quite many text that includes a number (digit).  \n",
        "In one of the pre-processing steps, we removed all the words/text that are only digits, but not combined.  \n",
        "You might want to remove these as well...  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSgn4DPGqtEz",
        "colab_type": "text"
      },
      "source": [
        "A comparison of TF-IDF values with respect to lemmatized sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vUTSbDdk0wL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c67ae3c5-418c-468a-e598-e19c09408040"
      },
      "source": [
        "print(df['sentence_lemmatized'].head())"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0                                      wow loved place\n",
            "1                                           crust good\n",
            "2                                  tasty texture nasty\n",
            "3    stopped late may bank holiday rick steve recom...\n",
            "4                           selection menu great price\n",
            "Name: sentence_lemmatized, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb7xxL-LlIul",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "4976f96f-5f4e-41cf-f646-1f635733e641"
      },
      "source": [
        "print(tf_idf[:5])"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 3090)\t0.41273224174375084\n",
            "  (0, 2480)\t0.5868528998051764\n",
            "  (0, 4739)\t0.6966030222554546\n",
            "  (1, 1812)\t0.4173052556395464\n",
            "  (1, 996)\t0.9087663746065944\n",
            "  (2, 2751)\t0.5937263288143424\n",
            "  (2, 4186)\t0.611851400615606\n",
            "  (2, 4150)\t0.5226154513955407\n",
            "  (3, 3369)\t0.3349134225950607\n",
            "  (3, 3980)\t0.3349134225950607\n",
            "  (3, 3483)\t0.365731635496751\n",
            "  (3, 2000)\t0.365731635496751\n",
            "  (3, 324)\t0.365731635496751\n",
            "  (3, 2570)\t0.28606771080664056\n",
            "  (3, 2351)\t0.3349134225950607\n",
            "  (3, 3992)\t0.3349134225950607\n",
            "  (3, 2480)\t0.2611864749314417\n",
            "  (4, 3199)\t0.47390012445546686\n",
            "  (4, 1842)\t0.34087140069306393\n",
            "  (4, 2611)\t0.5588379692363954\n",
            "  (4, 3658)\t0.5890038067537281\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kE8A2SBmxdf",
        "colab_type": "text"
      },
      "source": [
        "In the feature vector row (e.g., (0, 4843)), the first digit refers to the sentence row (i.e., first datarow).  \n",
        "The second digit is the index of alphebitically ordered word list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eNVD0_wGYoP",
        "colab_type": "text"
      },
      "source": [
        "### Sentiment Analysis\n",
        "\n",
        "Sentiment analysis is basically the process of determining the attitude or the emotion of the writer, i.e., whether it is positive or negative or neutral.\n",
        "\n",
        "We will use the Textblob library. The sentiment function of textblob returns the polarity of the sentence, i.e., a float value which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7Y1RR38HFs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTyI82q-rRZM",
        "colab_type": "text"
      },
      "source": [
        "Derive sentiment of each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB_i-p0QG8WT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['sentiment'] = df['sentence_lemmatized'].apply(lambda x: TextBlob(x).sentiment.polarity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxndvXWrrbdK",
        "colab_type": "text"
      },
      "source": [
        "Display original sentece with respect to its sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBZ-FeKdG8c2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "0bacc9b1-234f-421e-8b4d-45a78064538b"
      },
      "source": [
        "print(df[['sentence_lemmatized', 'sentiment']][:40])"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                  sentence_lemmatized  sentiment\n",
            "0                                     wow loved place   0.400000\n",
            "1                                          crust good   0.700000\n",
            "2                                 tasty texture nasty  -1.000000\n",
            "3   stopped late may bank holiday rick steve recom...   0.200000\n",
            "4                          selection menu great price   0.800000\n",
            "5                         getting angry want damn pho  -0.500000\n",
            "6                          honeslty didnt taste fresh   0.300000\n",
            "7   potato like rubber could tell made ahead time ...   0.000000\n",
            "8                                           fry great   0.800000\n",
            "9                                         great touch   0.800000\n",
            "10                                     service prompt   0.000000\n",
            "11                                      would go back   0.000000\n",
            "12  cashier care ever say still ended wayyy overpr...   0.000000\n",
            "13    tried cape cod ravoli chickenwith cranberrymmmm   0.000000\n",
            "14                   disgusted pretty sure human hair  -0.062500\n",
            "15                         shocked sign indicate cash  -0.700000\n",
            "16                                 highly recommended   0.160000\n",
            "17                       waitress little slow service  -0.243750\n",
            "18                    place worth time let alone vega   0.300000\n",
            "19                                               like   0.000000\n",
            "20                                     burrittos blah   0.000000\n",
            "21                                       food amazing   0.600000\n",
            "22                                  service also cute   0.500000\n",
            "23                   could care le interior beautiful   0.850000\n",
            "24                                          performed   0.000000\n",
            "25      thats rightthe red velvet cakeohhh stuff good   0.350000\n",
            "26                                               name   0.000000\n",
            "27  hole wall great mexican street taco friendly s...   0.391667\n",
            "28  took hour get food table restaurant food luke ...   0.300000\n",
            "29                               worst salmon sashimi  -1.000000\n",
            "30        also combo like burger fry beer decent deal   0.166667\n",
            "31                                    like final blow   0.000000\n",
            "32                 found place accident could happier   0.000000\n",
            "33  seems like good quick place grab bite familiar...   0.469444\n",
            "34                             overall like place lot   0.000000\n",
            "35           redeeming quality restaurant inexpensive   0.500000\n",
            "36                           ample portion good price   0.700000\n",
            "37  poor service waiter made feel like stupid ever...  -0.600000\n",
            "38                           first visit hiro delight   0.250000\n",
            "39                                       service suck   0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acrCV8JVVjdx",
        "colab_type": "text"
      },
      "source": [
        "## Text Classification\n",
        "\n",
        "We will explore few text classification approaches to classify the review data as either positive (1) or negative (0).  \n",
        "Here we will only use the amazon reviews (1000 reviews) for the workshop. (You may use yelp and imdb review data seperately evaluate the approaches.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSsjUBYZrl-y",
        "colab_type": "text"
      },
      "source": [
        "Previously, we conducted all the pre-processing steps to the entire 3 datasets (amazon, yelp and imdb). This for text classification we will filter only the reviews from amazon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl8comQ3V0zJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_amazon = df.loc[df['source'] == 'amazon']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBIhvHkmWdku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "2b08f088-2213-4248-e7bc-262a6cc6d3a9"
      },
      "source": [
        "df_amazon.head()"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>source</th>\n",
              "      <th>word_count_function</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>avg_word</th>\n",
              "      <th>sentence_stemmed</th>\n",
              "      <th>sentence_lemmatized</th>\n",
              "      <th>3_grams</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>way plug us unless go converter</td>\n",
              "      <td>0</td>\n",
              "      <td>amazon</td>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "      <td>82</td>\n",
              "      <td>2.952381</td>\n",
              "      <td>way plug us unless go convert</td>\n",
              "      <td>way plug u unless go converter</td>\n",
              "      <td>[way plug us, plug us unless, us unless go, un...</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1001</th>\n",
              "      <td>good case excellent value</td>\n",
              "      <td>1</td>\n",
              "      <td>amazon</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>27</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>good case excel valu</td>\n",
              "      <td>good case excellent value</td>\n",
              "      <td>[good case excellent, case excellent value]</td>\n",
              "      <td>0.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1002</th>\n",
              "      <td>great jawbone</td>\n",
              "      <td>1</td>\n",
              "      <td>amazon</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>22</td>\n",
              "      <td>4.750000</td>\n",
              "      <td>great jawbon</td>\n",
              "      <td>great jawbone</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1003</th>\n",
              "      <td>tied charger conversations lasting minutesmajo...</td>\n",
              "      <td>0</td>\n",
              "      <td>amazon</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>79</td>\n",
              "      <td>6.272727</td>\n",
              "      <td>tie charger convers last minutesmajor problem</td>\n",
              "      <td>tied charger conversation lasting minutesmajor...</td>\n",
              "      <td>[tied charger conversations, charger conversat...</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1004</th>\n",
              "      <td>mic great</td>\n",
              "      <td>1</td>\n",
              "      <td>amazon</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>17</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>mic great</td>\n",
              "      <td>mic great</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  ...  sentiment\n",
              "1000                    way plug us unless go converter  ...       0.00\n",
              "1001                          good case excellent value  ...       0.85\n",
              "1002                                      great jawbone  ...       0.80\n",
              "1003  tied charger conversations lasting minutesmajo...  ...       0.00\n",
              "1004                                          mic great  ...       0.80\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0JIXz4gr4A0",
        "colab_type": "text"
      },
      "source": [
        "Split train/validation data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hNVAhPGXmSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(df_amazon['sentence_lemmatized'], df_amazon['label'], test_size=0.3, random_state=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19jv2X2kZV3B",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kPqYktYsOxS",
        "colab_type": "text"
      },
      "source": [
        "We will use the Bag of Words model as text features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOLyQxTqWfWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aw3BCr0sYLQ",
        "colab_type": "text"
      },
      "source": [
        "Construct the bag of words model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgvIlfHhsYby",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8441b287-ad3d-49f7-b622-574a7a67634b"
      },
      "source": [
        "bow = CountVectorizer(min_df=0, lowercase=False)\n",
        "bow.fit(sentences_train)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=False, max_df=1.0, max_features=None, min_df=0,\n",
              "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mj1IC8CsgTW",
        "colab_type": "text"
      },
      "source": [
        "Fit the train and test sentences to transform them to bag-of-word features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNZIJTzdXCbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = vectorizer.transform(sentences_train)\n",
        "X_test  = vectorizer.transform(sentences_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ3_5XCOspgD",
        "colab_type": "text"
      },
      "source": [
        "Use a logistic regression model for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xqy_u4hYT2i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d33b1962-fe7a-4a0f-8d25-e6f518e0b0a9"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvhA_f2FtDeU",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhqBSmAfYg6I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "afe00557-4f19-44f8-ed88-3d05af52c948"
      },
      "source": [
        "score_train = classifier.score(X_train, y_train)\n",
        "score_test = classifier.score(X_test, y_test)\n",
        "print('Train accuracy: {:.2f}%'.format(score_train*100))\n",
        "print('Test accuracy: {:.2f}%'.format(score_test*100))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy: 95.86%\n",
            "Test accuracy: 78.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geKjAtjGtJ6g",
        "colab_type": "text"
      },
      "source": [
        "What can you say about bias and variance of this model?  \n",
        "- Low bias and high variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shvn4nHzhIRr",
        "colab_type": "text"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "You may conduct similar classification exercises for yelp and imdb datasets."
      ]
    }
  ]
}