{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 06 - NLP Advance Analytics Workshop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxb_OCgfN6I8",
        "colab_type": "text"
      },
      "source": [
        "# NLP Advanced Analytics Workshop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzMpphje4U3j",
        "colab_type": "text"
      },
      "source": [
        "In this workshop, we will conduct online text analysis and topic modeling using tweets that mention two major telecom providers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhaVRYP5N3vv",
        "colab_type": "text"
      },
      "source": [
        "**Installing required libraries.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrrrREeF-gbP",
        "colab_type": "text"
      },
      "source": [
        "Install pyLDAvis library for interactive topic visualization dashboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ySQ6wGP-e6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdLhxDT88dsF",
        "colab_type": "text"
      },
      "source": [
        "Load libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA3kUCEPlzBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading primary libraries.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# For text processing\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "import gensim\n",
        "import logging\n",
        "import tempfile\n",
        "from gensim import corpora, models, similarities\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pyLDAvis.gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg7fXsw_OiAq",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4mqofaBOMj-",
        "colab_type": "text"
      },
      "source": [
        "Load Twitter data of telstra and optus telecommunication providers. (Can be downloaded from the LMS)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggk1_jqGtJzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_telstra = pd.read_csv('tp7_telstra.csv')\n",
        "df_optus = pd.read_csv('tp7_optus.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqfRa-gAtTjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# display the column names of the datasets\n",
        "df_optus.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux_gSRtH3xPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_telstra.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3efEuyf-Csj",
        "colab_type": "text"
      },
      "source": [
        "As the first step, we can subset columns that are requried for our analysis. This can be done based on your business requirement.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWnj6q_Tta9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_telstra = df_telstra[['day', 'content', 'name', 'location']]\n",
        "df_optus = df_optus[['day', 'content', 'name', 'location']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPvW79i2tz0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_optus.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC5Z0YfV-aZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_telstra.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpH6gFKSt9xQ",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Urm5f0VCPEyO",
        "colab_type": "text"
      },
      "source": [
        "The preprocessing steps are needed to apply both the datasets.  It would require duplicated work - as we have to do each step twice (for 2 datasets).  \n",
        "Therefore, we will combine two datasets, while keeping track of the original dataset. For that, we create a seperate column ('flag') in both datasets, named 'provider' to indicate the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZKfrw-6uK6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new flag column for both datasets\n",
        "df_telstra['provider'] = 'telstra'\n",
        "df_optus['provider'] = 'optus'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Algep-z6PTOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combine the 2 datasets\n",
        "df = pd.concat([df_telstra, df_optus], ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8eSDSsjPcnu",
        "colab_type": "text"
      },
      "source": [
        "Now we conduct the pre-processing steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg6lzQo-t2bT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform sentences into lowercase\n",
        "df['content'] = df['content'].str.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrlDlSyYWJZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Following command is just to visualize the processed dataframe\n",
        "df.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwHNr_qP6aOe",
        "colab_type": "text"
      },
      "source": [
        "Remove twitter user ids that are mentioned in the tweet text. We will use [regular expressions ](https://www.w3schools.com/python/python_regex.asp) to do this.  \n",
        "In applying the regular expression, we use the string in the format r\"regular-expression\". This is to treat the regex as a raw string. Additional information on this can be found from [this stackoverflow question](https://stackoverflow.com/questions/4780088/what-does-preceding-a-string-literal-with-r-mean). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdYVnuwh6jSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_twitter_ids(tweet):\n",
        "  mention_removed_tweet = re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", str(tweet)) # remove any sequence of characters followed by '@' sign\n",
        "  spaces_removed = re.sub(r\"\\s\\s+\", \" \", str(mention_removed_tweet)) # remove multiple spaces\n",
        "  return spaces_removed\n",
        "\n",
        "df['content'] = df['content'].apply(remove_twitter_ids)\n",
        "df.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kScx1aPgxKoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How does above function works?\n",
        "# Take any tweet that has mentions in it. e.g., https://twitter.com/DavidLKeating/status/1251253645431144457\n",
        "twt = \"Canada just ordered #masks4all for flights. We need this on all US flights NOW. What are we waiting for @SecElaineChao @AmericanAir @Delta @SouthwestAir @united @AlaskaAir @FAANews ??? @jeremyphoward update\"\n",
        "out_twt = remove_twitter_ids(twt)\n",
        "print(out_twt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhHVeVrZUH3y",
        "colab_type": "text"
      },
      "source": [
        "### Duplicate removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMRw11QlR5Sx",
        "colab_type": "text"
      },
      "source": [
        "It is essential we check for duplicated tweets. Because, the tweets are extracted from Twitter API and it is likely the same tweet is captured multiple times.  \n",
        "We can utilize pandas drop_duplicates() function. [Link to API](https://pandas.pydata.org/pandas-docs/version/0.24.2/reference/api/pandas.DataFrame.drop_duplicates.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOLpxscovYup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check duplicate tweets count\n",
        "duplicate_count = len(df['content'])-len(df['content'].drop_duplicates())\n",
        "print('duplicate count:', duplicate_count)\n",
        "print('total records before remove duplicates:', df.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhfpF8q0wGpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drop duplicates (keep the last tweet of each of the duplicates)\n",
        "df = df.drop_duplicates(subset='content', keep=\"last\")\n",
        "print('updated record count:', df.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxPxafb-Sbqv",
        "colab_type": "text"
      },
      "source": [
        "Now we will remove all the punctuation marks keeping only the text.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej_QXjTdzvlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove punctuations\n",
        "df['content'] = df['content'].str.replace(r'[^\\w\\s]','')  # This is the use of regular expressions.\n",
        "df.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGHKq7N0ULpa",
        "colab_type": "text"
      },
      "source": [
        "### Stop words and domain related word removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifUgYivdSo-E",
        "colab_type": "text"
      },
      "source": [
        "Now let's conduct further text pre- processing using NLTK library as we did in the first NLP workshop. First, remove stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi11Id_j0moK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Remove stop words\n",
        "# Load NLTK library\n",
        "import nltk\n",
        "\n",
        "# Download the stopwords to the nltk library\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the stopwords\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2ntm-gP0z45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the list of all stopwords from the library\n",
        "stop = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDb3P9rNU6ih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(stop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UwtkKRr5ZRC",
        "colab_type": "text"
      },
      "source": [
        "Note that the term 'not' is in the stop word list. This will affect if our analysis is on sentiment analysis. However, if the analysis on topic modeling, this might not be the case. You should know that this depends on your analytics goal.  \n",
        "For this case, we will remove the term 'not' from the stopword list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf2LDu1Z55S_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop.remove('not')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUh_ZT1h5_D2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(stop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq9P8X7p021N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove the words in 'stop' list\n",
        "def remove_stop_words(tweet):\n",
        "  tokens = tweet.split()\n",
        "  stop_removed_tokens = [t for t in tokens if t not in stop]\n",
        "  convert_to_string = \" \".join(stop_removed_tokens)\n",
        "  return convert_to_string\n",
        "  \n",
        "df['content'] = df['content'].apply(remove_stop_words)\n",
        "df.tail(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yFHuyzAUH89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How above function works?\n",
        "def remove_stop_words(tweet):\n",
        "  tokens = tweet.split()\n",
        "  stop_removed_tokens = [t for t in tokens if t not in stop]\n",
        "  convert_to_string = \" \".join(stop_removed_tokens)\n",
        "  return convert_to_string\n",
        "\n",
        "txt = \"you are the first person.\"\n",
        "remove_stop_words(txt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJmhNP0eUbuC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For your knowledge: Same above function can be written as a lambda function\n",
        "# df['content'] = df['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDIqHSJYVUq_",
        "colab_type": "text"
      },
      "source": [
        "### Standardization of tweets\n",
        "\n",
        "We shall use stemming to standardize tweets.  \n",
        "[Stemming](https://en.wikipedia.org/wiki/Stemming) is the removal of prefix, suffix etc, to derive the base form of a word. We will use the NLTK library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EglB3uXU0590",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stemming_function(sent):\n",
        "  word_list = sent.split()\n",
        "  stemmed_word_list = [stemmer.stem(word) for word in word_list]\n",
        "  stemmed_sentence = \" \".join(stemmed_word_list)\n",
        "  return stemmed_sentence\n",
        "\n",
        "df['content_stem'] = df['content'].apply(stemming_function)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgB-ETbVV16D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compare the content vs. stemmed content\n",
        "df[['content', 'content_stem']].tail(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igcfX4-YWRYG",
        "colab_type": "text"
      },
      "source": [
        "The stemming has turned term 'optus' to 'optu', thus, stemming has not deemed expected results in this context.  Thereby, we will go ahead with original content without any standardization.   \n",
        "You may attempt to lemmatize and see if you can imporve the results as an experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfRQvWfo2YzW",
        "colab_type": "text"
      },
      "source": [
        "## Word frequency analysis\n",
        "\n",
        "Let us look at the most occuring words and the least occuring words in our tweet dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXqlB-0c2avi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a word frequency series. (This is a pandas series)\n",
        "word_frequency = pd.Series(' '.join(df['content']).split()).value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbHCTcin2jS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the top 10 words (you can write either word_frequency[:10] or word_frequency[0:10]. Both give the same result.)\n",
        "word_frequency[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_1yDzFF251v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the top word counts \n",
        "\n",
        "word_count  = word_frequency\n",
        "word_count = word_count[:10,]\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(word_count.index, word_count.values, alpha=0.8)\n",
        "plt.title('Tweets in top 10 words')\n",
        "plt.ylabel('Number of Occurrences', fontsize=12)\n",
        "plt.xlabel('Word', fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF7lRqLLiEnL",
        "colab_type": "text"
      },
      "source": [
        "We will now visualize the text corpus that we created after pre-processing to get insights on the most frequently used words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtDnGlk1iG5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "from wordcloud import WordCloud"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7nJrzy1iLkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = list(df['content'])\n",
        "\n",
        "wordcloud = WordCloud(background_color='white', max_words=200, max_font_size=50, random_state=42).generate(str(corpus))\n",
        "\n",
        "fig = plt.figure(1)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RGQEkTyd6iy",
        "colab_type": "text"
      },
      "source": [
        "**Insight**  \n",
        "From the word cloud we could identify that terms telstra and optus has been mentioned frequently. But this is expected - not so much an insight. Therefore, you may remove high frequent yet non-insightful words alongside the twitter specific words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "updNp-iD-ib2",
        "colab_type": "text"
      },
      "source": [
        "### Remove common words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbLPCbpdsWPl",
        "colab_type": "text"
      },
      "source": [
        "Let's  remove context specific words and the most common words from the tweets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFd0BEn2Tk__",
        "colab_type": "text"
      },
      "source": [
        "The tweets extracted are from the hashtags @telstra and @optus, these keywords will appear in most tweets. Let's  add those keywords to the set of words to be removed.  \n",
        "We can extend the new_words list with those additional keywords, which you may think will be widely used due to the area/domain the tweets were extracted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWiEKkSJsVSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a list of custom stopwords\n",
        "new_words_to_remove = [\"pic\", \"twitter\", \"com\", \"telstra\", \"optus\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcRvJVEb-y5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove common words\n",
        "# We will use lambda function here.\n",
        "df['content'] = df['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in new_words_to_remove))\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWMzOsNmXV3x",
        "colab_type": "text"
      },
      "source": [
        "Exercise: try to replicate lambda function above as a python function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWe9F1sgXkIG",
        "colab_type": "text"
      },
      "source": [
        "We will re-attempt to compose a frequency appearing word list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gJYMFSNc-_mq",
        "colab": {}
      },
      "source": [
        "# Create a word frequency series. (This is a pandas series)\n",
        "word_frequency = pd.Series(' '.join(df['content']).split()).value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tiTeSy2k-_mt",
        "colab": {}
      },
      "source": [
        "# Look at the top 10 words (you can write either word_frequency[:10] or word_frequency[0:10]. Both give the same result.)\n",
        "word_frequency[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR74vc_AdsN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = list(df['content'])\n",
        "\n",
        "wordcloud = WordCloud(background_color='white', max_words=200, max_font_size=50, random_state=42).generate(str(corpus))\n",
        "\n",
        "fig = plt.figure(1)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFT8faxgkbdL",
        "colab_type": "text"
      },
      "source": [
        "## Bigrams and Trigrams\n",
        "\n",
        "The following commands will extract bigrams from the tweet dataset.  \n",
        "From here onwards, we will isolate tweets for each provider to conduct an comparative analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZONqTOUaBVEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Seperate the two datasets by using pandas filtering mechanism.\n",
        "df_telstra_processed = df.loc[df['provider'] == 'telstra']\n",
        "df_optus_processed = df.loc[df['provider'] == 'optus']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ycL9IN31Bna",
        "colab_type": "text"
      },
      "source": [
        "### Bigrams\n",
        "\n",
        "We will use CountVectorizer object from sklearn to generate bi-grams and tri-grams. [Link to API](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)  \n",
        "\n",
        "You may refer [this StackoverFlow explanation](https://stackoverflow.com/questions/24005762/understanding-the-ngram-range-argument-in-a-countvectorizer-in-sklearn) to understand ngram_range parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wT-k2DBztsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function will generate most frequently occuring Bi-grams\n",
        "def get_ngrams(corpus, ngram_range=(2, 2)):\n",
        "    \n",
        "    # Create CountVectorizer object from sklearn library with bigrams\n",
        "    vec1 = CountVectorizer(ngram_range=ngram_range, max_features=2000).fit(corpus)\n",
        "\n",
        "    # Create BoW feature representation using word frequency\n",
        "    bag_of_words = vec1.transform(corpus)\n",
        "\n",
        "    # compute sum of words\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "\n",
        "    # create (word, frequency) tuples for bigrams\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec1.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEVxzVpaakjV",
        "colab_type": "text"
      },
      "source": [
        "Let us see an example on how above function works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVa4NJakZXj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "txts = ['John likes to watch movies', 'Mary likes movies too', 'Mary also likes to watch football games']\n",
        "get_ngrams(txts, ngram_range=(1, 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVTfSo9hzuHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigrams_telstra = get_ngrams(df_telstra_processed['content'].tolist(), ngram_range=(2, 2))\n",
        "bigrams_optus = get_ngrams(df_optus_processed['content'].tolist(), ngram_range=(2, 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7Ff3Llma6_q",
        "colab_type": "text"
      },
      "source": [
        "Convert bigrams of both datasets to a dataframe with column names bi-gram and frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAvyAGOkz3ZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigrams_telstra_df = pd.DataFrame(bigrams_telstra)\n",
        "bigrams_telstra_df.columns=[\"Bi-gram\", \"Freq\"]\n",
        "\n",
        "bigrams_optus_df = pd.DataFrame(bigrams_optus)\n",
        "bigrams_optus_df.columns=[\"Bi-gram\", \"Freq\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1pYpiJ81TpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Barplot of most freq Bi-grams\n",
        "top_bigrams_to_show = 20\n",
        "\n",
        "sns.set(rc={'figure.figsize':(13,8)})\n",
        "h=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=bigrams_telstra_df[:top_bigrams_to_show])\n",
        "h.set_xticklabels(h.get_xticklabels(), rotation=45)  # here rotation parameter shows the angle of your x-axis labels\n",
        "plt.title('Telstra - Bigram Analysis')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS9-14nyS4zW",
        "colab_type": "text"
      },
      "source": [
        "**Insights**:  \n",
        "The bigram 'data day' refers to an event. After a series of outages, Telstra declared a free data day as a compensation. Notice how bigrams can be used to detect such events as well as topics/themes specific to the domain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVDjNeb9z3d6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Barplot of most freq Bi-grams for optus\n",
        "top_bigrams_to_show = 20\n",
        "\n",
        "sns.set(rc={'figure.figsize':(13,8)})\n",
        "h=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=bigrams_optus_df[:top_bigrams_to_show])\n",
        "h.set_xticklabels(h.get_xticklabels(), rotation=45)\n",
        "plt.title('Optus - Bigram Analysis')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qg7sg1VTe6L",
        "colab_type": "text"
      },
      "source": [
        "**Insights:**  \n",
        "Observe that in Optus dataset, there are many words related to sport which were not prominent in Telstra data. Bigram frequencies can be used to differentiate and compare topics of interest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn_-jogT2fO-",
        "colab_type": "text"
      },
      "source": [
        "### Trigrams\n",
        "\n",
        "Let's now attempt trigrams. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aEBUbEv92d0k",
        "colab": {}
      },
      "source": [
        "#Most frequently occuring Tri-grams\n",
        "def get_trigrams(corpus):\n",
        "    vec1 = CountVectorizer(ngram_range=(3,3), max_features=2000).fit(corpus)\n",
        "    bag_of_words = vec1.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec1.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fqNtgml42d0n",
        "colab": {}
      },
      "source": [
        "trigrams_telstra = get_ngrams(df_telstra_processed['content'].tolist(), ngram_range=(3, 3))\n",
        "trigrams_optus = get_ngrams(df_optus_processed['content'].tolist(), ngram_range=(3, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOMzNZvhbjPy",
        "colab_type": "text"
      },
      "source": [
        "Convert tri-grams of both datasets to a dataframe with column names tri-gram and frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fmfQV3vw2d0p",
        "colab": {}
      },
      "source": [
        "trigrams_telstra_df = pd.DataFrame(trigrams_telstra)\n",
        "trigrams_telstra_df.columns=[\"Tri-gram\", \"Freq\"]\n",
        "\n",
        "trigrams_optus_df = pd.DataFrame(trigrams_optus)\n",
        "trigrams_optus_df.columns=[\"Tri-gram\", \"Freq\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QZ5GxQ1i2d0r",
        "colab": {}
      },
      "source": [
        "# Barplot of most freq Tri-grams\n",
        "top_trigrams_to_show = 20\n",
        "\n",
        "sns.set(rc={'figure.figsize':(13,8)})\n",
        "h=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=trigrams_telstra_df[:top_trigrams_to_show])\n",
        "h.set_xticklabels(h.get_xticklabels(), rotation=90)\n",
        "plt.title('Telstra - Trigram Analysis')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PmgL8bb-2d0y",
        "colab": {}
      },
      "source": [
        "# Barplot of most freq Tri-grams\n",
        "top_trigrams_to_show = 20\n",
        "\n",
        "sns.set(rc={'figure.figsize':(13,8)})\n",
        "h=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=trigrams_optus_df[:top_trigrams_to_show])\n",
        "h.set_xticklabels(h.get_xticklabels(), rotation=90)\n",
        "plt.title('Optus - Tri Analysis')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBtMfB2W7sI7",
        "colab_type": "text"
      },
      "source": [
        "Can you determine new insights from Trigram plots?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACoq6P-D5Ho7",
        "colab_type": "text"
      },
      "source": [
        "## Temporal Analysis\n",
        "\n",
        "Now let's attempt to explore temporal patterns in the two datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOICy21-9FBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Group the twitter datasets based on the date field and get the count per each day.\n",
        "df_telstra_date_wise = df_telstra_processed.groupby(['day'])['content'].count().reset_index(name='tweet_count_telstra').set_index('day')\n",
        "df_optus_date_wise = df_optus_processed.groupby(['day'])['content'].count().reset_index(name='tweet_count_optus').set_index('day')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb_BMx56Xpj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_telstra_date_wise.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmA_8jzTXyTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_optus_date_wise.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbPYdReUGPkL",
        "colab_type": "text"
      },
      "source": [
        "When plotting with python, we can change the size of the plot and save it as a figure in your workspace.  \n",
        "* Change size: plt.figure(figsize=(width, length)) # width and length are in inches  \n",
        "* Save the image: plt.savefig(filename, dpi)  # dpi is the resolution in pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYWtYZZb9FGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot twitter activity timeline.\n",
        "plt.figure(figsize=(10, 4))\n",
        "ax = df_telstra_date_wise.plot()\n",
        "df_optus_date_wise.plot(ax=ax)\n",
        "plt.xticks(np.arange(len(df_telstra_date_wise.index)), df_optus_date_wise.index, rotation=90)\n",
        "plt.xlabel('Date')\n",
        "plt.savefig('temporal_analysis.png', dpi=800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTcpY3p_G0Kc",
        "colab_type": "text"
      },
      "source": [
        "Output image is saved in files tab. You can right click on the image and download it to your computer for further analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq4-Euub8EU8",
        "colab_type": "text"
      },
      "source": [
        "Insights: How would you describe the two line plots? Any actionable insights?  \n",
        "Exerciese: Have a look on the Telstra dataset on 2018-01-28. What does this spike in tweet count says?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFxNx9scx94w",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis\n",
        "\n",
        "Analyse sentiments for each tweet, aggregate into the sentiment of particular day and derive a sentiment timeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmqx0IuNyikj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract sentiments from the tweets (This should take some time because we run this to the entire dataset.)\n",
        "df['sentiment'] = df['content'].apply(lambda x: TextBlob(x).sentiment.polarity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_1yMaTzywF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display first 50 tweets with respective sentiment value\n",
        "df[['content', 'sentiment']].head(50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JY5JOIWccqd",
        "colab_type": "text"
      },
      "source": [
        "Again we will conduct invididual analysis for both telcom providers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViQSrLgqzxmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_telstra_processed = df.loc[df['provider'] == 'telstra']\n",
        "df_optus_processed = df.loc[df['provider'] == 'optus']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc-P7I3X1S1y",
        "colab_type": "text"
      },
      "source": [
        "Aggregate the sentiment value (using mean) for each provider"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02__qOk5zaoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_senti_telstra_date_wise = df_telstra_processed.groupby(['day'])['sentiment'].mean().reset_index(name='mean_sentiment_telstra').set_index('day')\n",
        "df_senti_optus_date_wise = df_optus_processed.groupby(['day'])['sentiment'].mean().reset_index(name='mean_sentiment_optus').set_index('day')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzDqnheh1a-M",
        "colab_type": "text"
      },
      "source": [
        "Plot sentiment over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U6hN4srz9JU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot twitter sentimet timeline over each provider\n",
        "ax = df_senti_telstra_date_wise.plot()\n",
        "df_senti_optus_date_wise.plot(ax=ax)\n",
        "plt.xticks(np.arange(len(df_senti_telstra_date_wise.index)), df_senti_optus_date_wise.index, rotation=90)\n",
        "plt.xlabel('Date')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUPE7xLTURsA",
        "colab_type": "text"
      },
      "source": [
        "**Insights:**   \n",
        "When deriving insights you can refer to days with significant sentiment scores (peaks) and explore the tweets to identify what event has led to positive/negative sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo4jyAjqCHuU",
        "colab_type": "text"
      },
      "source": [
        "## Topic Modeling\n",
        "\n",
        "We will conduct a topic modeling for one of the twitter datasets here (Telstra).  \n",
        "It is possible to conduct topic modeling using single words, bigrams, trigrams or n-grams.  \n",
        "Will be using an topic modeling algorithm named - LDA (Latent Dirichlet Allocation) for this task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEI-l8rB4GFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting up the environment for LDA algorithm.\n",
        "\n",
        "TEMP_FOLDER = tempfile.gettempdir()\n",
        "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3c76ldM50h4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filter the dataset of the telstra only.\n",
        "df_telstra_processed = df.loc[df['provider'] == 'telstra']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3glWaokJcbrc",
        "colab_type": "text"
      },
      "source": [
        "Now we will create the text corpus.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj4SduU8CrvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the lemmatized tweets as the text corpus.\n",
        "corpus = list(df_telstra_processed['content'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7nM7f4NDEr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokanization\n",
        "telstra_texts = [[word for word in str(document).split()] for document in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8c-cdfIC2ZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dictionary based on the tokanized words of all the tweets.\n",
        "dictionary = corpora.Dictionary(telstra_texts)\n",
        "\n",
        "# Save the above dictionary as a local file for LDA model to access.\n",
        "dictionary.save(os.path.join(TEMP_FOLDER, 'telstra.dict'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhBK3pV9C3i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the dictionary\n",
        "print(dictionary.token2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKTfpgqXD0Sf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the text dictionary to bag of words model\n",
        "corpus = [dictionary.doc2bow(text) for text in telstra_texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oirxMJXkef42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Corpus in machine readable format.\n",
        "print(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5f1KJmWfQL9",
        "colab_type": "text"
      },
      "source": [
        "What did just happen above?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4iD2ydne10-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweet_id = 0\n",
        "print(telstra_texts[tweet_id]) # each tweet converted to tokens\n",
        "print(dictionary.doc2bow(telstra_texts[tweet_id])) # each token is represented as a id from a dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtMx8yD0exFJ",
        "colab_type": "text"
      },
      "source": [
        "### Generate the topic model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgDpv0O2dnQB",
        "colab_type": "text"
      },
      "source": [
        "Recall we initially loaded models library from gensim.  \n",
        "(from gensim import corpora, models, similarities)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WvyZ3knEIZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct TF-IDF features from the dictionary.\n",
        "tfidf = models.TfidfModel(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxwzSdbmEIeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform the tweets as TF-IDF feature vectors\n",
        "corpus_tfidf = tfidf[corpus] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy7v_W8NeAgG",
        "colab_type": "text"
      },
      "source": [
        "We need to define how many topics we capture through LDA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-Oi5FHKEIi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_topics = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGw-20N-fzxq",
        "colab_type": "text"
      },
      "source": [
        "Below code will build the LDA topic model.   \n",
        "We have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6Dzb0WXEIc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=total_topics)\n",
        "corpus_lda = lda[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKloeZC1gKEP",
        "colab_type": "text"
      },
      "source": [
        "The above LDA model is built with 10 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.  \n",
        "You can see the keywords for each topic and the weightage(importance) of each keyword using lda_model.print_topics() as shown next.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xni29CwgEPnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the Keyword in the 10 topics\n",
        "lda.show_topics(total_topics, num_words=6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p16etrugc8p",
        "colab_type": "text"
      },
      "source": [
        "How to interpret this?  \n",
        "Topic 5 is a represented as '0.020*\"nbn\" + 0.010*\"thanks\" + 0.009*\"turnbullmalcolm\" + 0.009*\"mobile\" + 0.009*\"copper\" + 0.008*\"buy\"'  \n",
        "  \n",
        "It means the top 6 keywords that contribute to this topic are: nbn, thanks, turnbullmalcolm, mobile, copper, buy and the weight of ‘nbn’ on topic 5 is 0.020.  \n",
        "  \n",
        "The weights reflect how important a keyword is to that topic.  \n",
        "\n",
        "Looking at these keywords, can you guess what this topic could be? You may summarise it as nbn connectivity.  \n",
        "\n",
        "Likewise, can you go through the remaining topic keywords and judge what the topic is?  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8HS5zkCe-TV",
        "colab_type": "text"
      },
      "source": [
        "### Interactive topic analyzer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_dxGdsbEeiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.gensim.prepare(lda, corpus_lda, dictionary, mds='tsne')\n",
        "panel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzAl4qwBIyob",
        "colab_type": "text"
      },
      "source": [
        "What is the relevance matrix? (The right side slider)  \n",
        "Understand what it means and how it can be used to derive more relevant topics from [this paper](https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOLO9ET15mxA",
        "colab_type": "text"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXN6o8SKD_7L",
        "colab_type": "text"
      },
      "source": [
        "1.   [LDA Topic Modeling](https://youtu.be/3mHy4OSyRf0)\n",
        "2.   [Topic Modelling in Python - Tutorial](https://ourcodingclub.github.io/2018/12/10/topic-modelling-python.html)"
      ]
    }
  ]
}